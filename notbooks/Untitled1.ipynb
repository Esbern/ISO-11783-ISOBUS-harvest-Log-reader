{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a553de8-b804-4a85-a3fd-da4f7865d6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Indexing Tasks...\n",
      "Found 142 tasks. Processing...\n",
      "Processing TLG00142...\n",
      "Done! Processed 141 files. Check: /Users/holmes/local_dev/agri_analysis/data/ENRICHED_SIMPLE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "import pyproj\n",
    "\n",
    "# --- CONFIGURATION -----------------------------------------------------------\n",
    "INPUT_FOLDER    = r'/Users/holmes/local_dev/agri_analysis/data/TASKDATA'\n",
    "OUTPUT_FOLDER   = r'/Users/holmes/local_dev/agri_analysis/data/ENRICHED_SIMPLE'\n",
    "MIN_LAT, MAX_LAT = 54.0, 58.0   # Denmark Latitude Bounds\n",
    "MIN_LON, MAX_LON = 8.0, 16.0    # Denmark Longitude Bounds\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# DDI Registry (The Dictionary of known sensors)\n",
    "DDI_MAP = {\n",
    "    '018D': 'Speed_M_S',           # Scaled to m/s\n",
    "    '0095': 'Yield_Vol_L_S',       # Scaled to L/s\n",
    "    '0054': 'Yield_Mass_Kg_S',     # Scaled to kg/s\n",
    "    '0063': 'Moisture_Pct',        # Scaled to %\n",
    "    '013A': 'Fuel_Rate_L_H',       # Scaled to L/h\n",
    "    '008D': 'Work_State',          # 0-100%\n",
    "    '0055': 'Crop_Temp_C',         # Celsius\n",
    "    'E122': 'Header_Status',       # Enum\n",
    "    '0074': 'Total_Area_Ha',       # Hectares\n",
    "    '0077': 'Duration_Sec',        # Seconds\n",
    "}\n",
    "\n",
    "def ensure_folder(path):\n",
    "    if not os.path.exists(path): os.makedirs(path)\n",
    "\n",
    "def parse_xml_structure(xml_path):\n",
    "    \"\"\"Reads the sidecar XML to find the list of sensors (DDIs).\"\"\"\n",
    "    ddis = []\n",
    "    if os.path.exists(xml_path):\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            # Find all Data Log Values (DLV) in the Time (TIM) block\n",
    "            for dlv in tree.findall(\".//DLV\"):\n",
    "                ddis.append(dlv.attrib.get('A'))\n",
    "        except: pass\n",
    "    \n",
    "    # Fallback default if XML is broken/missing\n",
    "    if not ddis: ddis = ['0054', '0095', '018D', '0063']\n",
    "    return ddis\n",
    "\n",
    "def detect_stride(content):\n",
    "    \"\"\"Finds the packet size by looking for the repeating Denmark coordinate signature.\"\"\"\n",
    "    offsets = []\n",
    "    limit = min(len(content), 200000) # Scan first 200KB\n",
    "    \n",
    "    # Header is always Time(4) + Lat(4) + Lon(4) = 12 bytes\n",
    "    for i in range(0, limit - 12):\n",
    "        try:\n",
    "            # Look for Lat/Lon at bytes 4-12 relative to start\n",
    "            lat, lon = struct.unpack('<ii', content[i+4:i+12])\n",
    "            lat, lon = lat * 1e-7, lon * 1e-7\n",
    "            if (MIN_LAT < lat < MAX_LAT) and (MIN_LON < lon < MAX_LON):\n",
    "                offsets.append(i)\n",
    "        except: pass\n",
    "\n",
    "    if len(offsets) < 10: return None\n",
    "    \n",
    "    # Find most common distance between headers\n",
    "    diffs = np.diff(offsets)\n",
    "    # Filter for realistic packet sizes (16 to 256 bytes)\n",
    "    valid_diffs = [d for d in diffs if 16 <= d <= 256]\n",
    "    if not valid_diffs: return None\n",
    "    \n",
    "    # Return the Mode (most frequent stride)\n",
    "    return max(set(valid_diffs), key=valid_diffs.count)\n",
    "\n",
    "def process_file(bin_path, xml_path, meta):\n",
    "    # 1. Setup\n",
    "    with open(bin_path, 'rb') as f: content = f.read()\n",
    "    \n",
    "    # 2. Auto-Detect Structure\n",
    "    stride = detect_stride(content)\n",
    "    if not stride: return False # Skip garbage files\n",
    "    \n",
    "    ddi_list = parse_xml_structure(xml_path)\n",
    "    payload_len = stride - 12\n",
    "    num_ints = payload_len // 4\n",
    "    \n",
    "    # 3. Extract Raw Data\n",
    "    rows = []\n",
    "    cursor = 0\n",
    "    while cursor < len(content) - stride:\n",
    "        try:\n",
    "            # Read Header\n",
    "            time_ms, lat_raw, lon_raw = struct.unpack('<Lii', content[cursor:cursor+12])\n",
    "            lat, lon = lat_raw * 1e-7, lon_raw * 1e-7\n",
    "            \n",
    "            # Geo-Check\n",
    "            if (MIN_LAT < lat < MAX_LAT) and (MIN_LON < lon < MAX_LON):\n",
    "                # Read Payload\n",
    "                p_start = cursor + 12\n",
    "                # Unpack as generic integers\n",
    "                values = struct.unpack(f'<{num_ints}I', content[p_start : p_start + (num_ints*4)])\n",
    "                \n",
    "                row = {'Time_Raw': time_ms, 'Latitude': lat, 'Longitude': lon}\n",
    "                \n",
    "                # Smart Mapping: Map known DDIs to values\n",
    "                # Case A: Forensic 31-byte format (Compact)\n",
    "                if stride == 31:\n",
    "                    # Specific offsets for TLG00001 style files\n",
    "                    payload_bytes = content[p_start : p_start+19]\n",
    "                    row['Yield_Mass_Kg_S'] = struct.unpack('<I', payload_bytes[2:6])[0]\n",
    "                    row['Speed_M_S']       = struct.unpack('<H', payload_bytes[10:12])[0]\n",
    "                    row['Moisture_Pct']    = struct.unpack('<H', payload_bytes[15:17])[0]\n",
    "                \n",
    "                # Case B: Standard XML format\n",
    "                else:\n",
    "                    for i, val in enumerate(values):\n",
    "                        if i < len(ddi_list):\n",
    "                            code = ddi_list[i]\n",
    "                            name = DDI_MAP.get(code, f\"DDI_{code}\")\n",
    "                            row[name] = val\n",
    "                            \n",
    "                rows.append(row)\n",
    "                cursor += stride # Jump to next\n",
    "            else:\n",
    "                cursor += 1 # Scan forward\n",
    "        except: cursor += 1\n",
    "            \n",
    "    if not rows: return False\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # 4. Clean & Scale Values\n",
    "    # Speed\n",
    "    if 'Speed_M_S' in df.columns:\n",
    "        s = df['Speed_M_S'].fillna(0)\n",
    "        # Fix unsigned wrap noise (>2B) and scale mm/s -> m/s\n",
    "        s = np.where(s > 2000000000, 0, s) * 0.001 \n",
    "        df['Speed_M_S'] = s\n",
    "    else: df['Speed_M_S'] = 2.0 # Default\n",
    "\n",
    "    # Yield\n",
    "    if 'Yield_Mass_Kg_S' in df.columns:\n",
    "        m = df['Yield_Mass_Kg_S'].fillna(0)\n",
    "        m = np.where(m > 2000000000, 0, m) * 0.000001 # mg/s -> kg/s\n",
    "        df['Yield_Mass_Kg_S'] = m\n",
    "    else: df['Yield_Mass_Kg_S'] = 0\n",
    "\n",
    "    # Moisture\n",
    "    if 'Moisture_Pct' in df.columns:\n",
    "        mst = df['Moisture_Pct'].fillna(0)\n",
    "        mst = np.where(mst > 2000000000, 0, mst) * 0.0001 * 100 # ppm -> %\n",
    "        df['Moisture_Pct'] = mst\n",
    "    else:\n",
    "        df['Moisture_Pct'] = 0.0 # Force creation if missing\n",
    "\n",
    "    # 5. Physics-Based Time & Yield Recalculation\n",
    "    # Project Lat/Lon to Meters (UTM32N)\n",
    "    transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", \"EPSG:25832\", always_xy=True)\n",
    "    xx, yy = transformer.transform(df['Longitude'].values, df['Latitude'].values)\n",
    "    \n",
    "    # Calculate Dist & Time\n",
    "    dist = np.sqrt(np.diff(xx, prepend=xx[0])**2 + np.diff(yy, prepend=yy[0])**2)\n",
    "    # Avoid Div/0: Minimum speed 0.1 m/s\n",
    "    speed = df['Speed_M_S'].clip(lower=0.1) \n",
    "    dt = dist / speed\n",
    "    dt = np.clip(dt, 0, 15) # Cap gaps at 15s\n",
    "    \n",
    "    # Reconstruct Timeline\n",
    "    start_dt = meta.get('Start', datetime(2024,1,1))\n",
    "    df['Datetime'] = start_dt + pd.to_timedelta(np.cumsum(dt), unit='s')\n",
    "    \n",
    "    # Calculate Yield (t/ha)\n",
    "    # (kg/s * 10) / (m/s * width_m)\n",
    "    HEADER_WIDTH = 9.0 \n",
    "    df['Yield_T_Ha'] = (df['Yield_Mass_Kg_S'] * 10.0) / (speed * HEADER_WIDTH)\n",
    "    df['Yield_T_Ha'] = df['Yield_T_Ha'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # 6. Save\n",
    "    year = meta.get('Year', 2024)\n",
    "    # Sanitize Filename\n",
    "    def clean(s): return str(s).strip().replace(' ', '_').replace('/', '-').replace(':', '')\n",
    "    \n",
    "    fname = f\"{year}_{clean(meta['Field'])}_{clean(meta['Crop'])}.csv\"\n",
    "    if 'Import' in fname: fname = f\"{year}_{clean(meta['LogID'])}.csv\"\n",
    "    \n",
    "    out_dir = os.path.join(OUTPUT_FOLDER, str(year))\n",
    "    ensure_folder(out_dir)\n",
    "    \n",
    "    # Handle collisions\n",
    "    full_path = os.path.join(out_dir, fname)\n",
    "    if os.path.exists(full_path):\n",
    "        fname = fname.replace('.csv', f\"_{clean(meta['LogID'])}.csv\")\n",
    "        full_path = os.path.join(out_dir, fname)\n",
    "        \n",
    "    # Select columns - FIX: Ensure all columns exist\n",
    "    cols = ['Datetime', 'Latitude', 'Longitude', 'Yield_T_Ha', 'Speed_M_S', 'Moisture_Pct', 'Yield_Mass_Kg_S']\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "            \n",
    "    df[cols].to_csv(full_path, index=False)\n",
    "    return True\n",
    "\n",
    "# --- MAIN EXECUTION ----------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    ensure_folder(OUTPUT_FOLDER)\n",
    "    \n",
    "    print(\"1. Indexing Tasks...\")\n",
    "    # Parse TASKDATA.XML for Metadata\n",
    "    tasks = []\n",
    "    try:\n",
    "        tree = ET.parse(os.path.join(INPUT_FOLDER, 'TASKDATA.XML'))\n",
    "        products = {p.attrib.get('A'): p.attrib.get('B') for p in tree.findall(\".//PDT\")}\n",
    "        fields = {f.attrib.get('A'): f.attrib.get('C') for f in tree.findall(\".//PFD\")}\n",
    "        \n",
    "        for tsk in tree.findall(\".//TSK\"):\n",
    "            tlg = tsk.find(\"TLG\")\n",
    "            if tlg is None: continue\n",
    "            log_id = tlg.attrib.get('A')\n",
    "            \n",
    "            # Crop & Field\n",
    "            crop_id = tsk.find(\"PAN\").attrib.get('A') if tsk.find(\"PAN\") is not None else \"\"\n",
    "            crop = products.get(crop_id, \"Unknown\")\n",
    "            field = fields.get(tsk.attrib.get('E'), \"Unknown\")\n",
    "            \n",
    "            # Time\n",
    "            s_time = tsk.find(\"TIM\").attrib.get('A') if tsk.find(\"TIM\") is not None else \"\"\n",
    "            try: start_dt = datetime.fromisoformat(s_time.replace('Z',''))\n",
    "            except: start_dt = datetime(2024,1,1)\n",
    "            \n",
    "            tasks.append({\n",
    "                'LogID': log_id,\n",
    "                'BinPath': os.path.join(INPUT_FOLDER, log_id + '.bin'),\n",
    "                'XmlPath': os.path.join(INPUT_FOLDER, log_id + '.xml'),\n",
    "                'Year': start_dt.year,\n",
    "                'Start': start_dt,\n",
    "                'Crop': crop,\n",
    "                'Field': field\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading TASKDATA.XML: {e}\")\n",
    "\n",
    "    print(f\"Found {len(tasks)} tasks. Processing...\")\n",
    "    \n",
    "    count = 0\n",
    "    for t in tasks:\n",
    "        if os.path.exists(t['BinPath']):\n",
    "            print(f\"Processing {t['LogID']}...\", end='\\r')\n",
    "            if process_file(t['BinPath'], t['XmlPath'], t):\n",
    "                count += 1\n",
    "                \n",
    "    print(f\"\\nDone! Processed {count} files. Check: {OUTPUT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db48c26-2b64-4d99-801a-1fc82a1e0325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
