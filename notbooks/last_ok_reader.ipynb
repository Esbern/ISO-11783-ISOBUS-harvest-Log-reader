{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccdcc342-14de-4be6-a85f-ccd10ff520c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. BUILDING INDEX ---\n",
      "Found 142 tasks.\n",
      "\n",
      "--- 2. PROCESSING ---\n",
      "  -> Warning: Auto-detect failed. Using XML stride: 68\n",
      "\n",
      "Done. 142 files enriched in '/Users/holmes/local_dev/agri_analysis/data/ENRICHED4'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pyproj\n",
    "from collections import Counter\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "DATA_FOLDER     = r'/Users/holmes/local_dev/agri_analysis/data/TASKDATA'\n",
    "INTERIM_FOLDER  = r'/Users/holmes/local_dev/agri_analysis/data/taskdata_out2'\n",
    "ENRICHED_FOLDER = r'/Users/holmes/local_dev/agri_analysis/data/ENRICHED4'\n",
    "\n",
    "# \"Denmark Signature\" - Broad bounds to validate coordinates\n",
    "MIN_LAT, MAX_LAT = 54.0, 58.0\n",
    "MIN_LON, MAX_LON = 8.0, 16.0\n",
    "\n",
    "for f in [INTERIM_FOLDER, ENRICHED_FOLDER]:\n",
    "    if not os.path.exists(f): os.makedirs(f)\n",
    "\n",
    "# ==========================================\n",
    "# 1. ROBUST STRIDE DETECTOR (The Fix)\n",
    "# ==========================================\n",
    "def detect_stride_robust(content):\n",
    "    \"\"\"\n",
    "    Scans the ENTIRE binary to find the repeating pattern distance.\n",
    "    \"\"\"\n",
    "    total_len = len(content)\n",
    "    # We scan for Lat/Lon because they are distinct (54-58, 8-16)\n",
    "    # Header is Time(4) + Lat(4) + Lon(4)\n",
    "    \n",
    "    valid_offsets = []\n",
    "    \n",
    "    # Scan with a step of 1 to find every possible valid coordinate header\n",
    "    # Limit scan to first 500KB to be fast, but enough to find pattern\n",
    "    scan_limit = min(total_len, 500000) \n",
    "    \n",
    "    for i in range(0, scan_limit - 12):\n",
    "        try:\n",
    "            # Look for Lat/Lon at offsets 4 and 8 relative to 'i'\n",
    "            # (Assuming 'i' is the start of the 12-byte header)\n",
    "            lat_raw, lon_raw = struct.unpack('<ii', content[i+4:i+12])\n",
    "            \n",
    "            lat = lat_raw * 1e-7\n",
    "            lon = lon_raw * 1e-7\n",
    "            \n",
    "            # Check Signature\n",
    "            if (MIN_LAT < lat < MAX_LAT) and (MIN_LON < lon < MAX_LON):\n",
    "                valid_offsets.append(i)\n",
    "        except: pass\n",
    "        \n",
    "    if len(valid_offsets) < 10:\n",
    "        return None \n",
    "    \n",
    "    # Calculate distances between ALL found valid headers\n",
    "    diffs = np.diff(valid_offsets)\n",
    "    \n",
    "    # Filter for realistic strides (e.g., 16 bytes to 256 bytes)\n",
    "    # A stride of 1 or 4 is likely a false positive coincidence\n",
    "    valid_diffs = [d for d in diffs if 16 <= d <= 256]\n",
    "    \n",
    "    if not valid_diffs: return None\n",
    "    \n",
    "    # Find the most common difference (The Stride)\n",
    "    mode_stride = Counter(valid_diffs).most_common(1)[0][0]\n",
    "    return mode_stride\n",
    "\n",
    "# ==========================================\n",
    "# 2. UNIVERSAL EXTRACTOR\n",
    "# ==========================================\n",
    "def extract_file(bin_path, out_csv_path, xml_ddi_list):\n",
    "    if not os.path.exists(bin_path): return False\n",
    "    \n",
    "    with open(bin_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # A. Detect Stride\n",
    "    stride = detect_stride_robust(content)\n",
    "    \n",
    "    # B. Fallback if detection fails\n",
    "    if not stride:\n",
    "        # Fallback to XML definition: 12 byte header + 4 bytes per sensor\n",
    "        stride = 12 + (len(xml_ddi_list) * 4)\n",
    "        print(f\"  -> Warning: Auto-detect failed. Using XML stride: {stride}\")\n",
    "    else:\n",
    "        # print(f\"  -> Auto-detected Stride: {stride}\") # Optional debug\n",
    "        pass\n",
    "        \n",
    "    # Calculate Payload\n",
    "    payload_size = stride - 12\n",
    "    num_ints = payload_size // 4\n",
    "    \n",
    "    # Prepare DDI Columns\n",
    "    # We map the XML DDI list to the first N integers we find\n",
    "    col_names = []\n",
    "    # Registry of known codes\n",
    "    DDI_MAP = {\n",
    "        '0054': 'Yield_Mass', '0095': 'Yield_Vol', '018D': 'Speed', \n",
    "        '0063': 'Moisture', '0053': 'Dry_Mass', '013A': 'Fuel'\n",
    "    }\n",
    "    \n",
    "    for ddi in xml_ddi_list:\n",
    "        col_names.append(DDI_MAP.get(ddi, f\"DDI_{ddi}\"))\n",
    "    \n",
    "    # Extraction Loop\n",
    "    valid_rows = []\n",
    "    cursor = 0\n",
    "    total_len = len(content)\n",
    "    \n",
    "    while cursor < total_len - stride:\n",
    "        try:\n",
    "            # 1. Validate Header\n",
    "            _, lat_raw, lon_raw = struct.unpack('<Lii', content[cursor:cursor+12])\n",
    "            lat = lat_raw * 1e-7\n",
    "            lon = lon_raw * 1e-7\n",
    "            \n",
    "            if (MIN_LAT < lat < MAX_LAT) and (MIN_LON < lon < MAX_LON):\n",
    "                # 2. Extract Data\n",
    "                time_ms = struct.unpack('<L', content[cursor:cursor+4])[0]\n",
    "                \n",
    "                # Extract Payload as Integers\n",
    "                p_start = cursor + 12\n",
    "                # Only read as many ints as fit in the payload\n",
    "                vals = struct.unpack(f'<{num_ints}I', content[p_start : p_start+(num_ints*4)])\n",
    "                \n",
    "                row = {'Time_ms': time_ms, 'Latitude': lat, 'Longitude': lon}\n",
    "                \n",
    "                # 3. Smart Mapping\n",
    "                # If we have a Stride of 31 (19b payload), we know it's the Compact Format\n",
    "                # regardless of what the XML says.\n",
    "                if stride == 31:\n",
    "                    # Forensic Mapping (Offsets relative to payload start)\n",
    "                    # Payload: [0-3]?, [4-7]?, [8-11]?, [12-15]?, [16-18]?\n",
    "                    # Based on TLG00001 analysis:\n",
    "                    # Mass @ 2, Speed @ 10, Moist @ 15\n",
    "                    payload_bytes = content[p_start : p_start+19]\n",
    "                    row['Raw_Yield_Mass'] = struct.unpack('<I', payload_bytes[2:6])[0]\n",
    "                    row['Raw_Speed']      = struct.unpack('<H', payload_bytes[10:12])[0]\n",
    "                    row['Raw_Moisture']   = struct.unpack('<H', payload_bytes[15:17])[0]\n",
    "                else:\n",
    "                    # Standard Mapping (Use XML order)\n",
    "                    for i, val in enumerate(vals):\n",
    "                        # Map to DDI name if we have one, else Val_X\n",
    "                        if i < len(col_names):\n",
    "                            name = col_names[i]\n",
    "                            # Heuristic: If name is Speed, store as Raw_Speed\n",
    "                            if 'Speed' in name: row['Raw_Speed'] = val\n",
    "                            elif 'Yield_Mass' in name: row['Raw_Yield_Mass'] = val\n",
    "                            elif 'Moisture' in name: row['Raw_Moisture'] = val\n",
    "                            else: row[name] = val\n",
    "                        else:\n",
    "                            row[f'Val_{i}'] = val\n",
    "                            \n",
    "                valid_rows.append(row)\n",
    "                cursor += stride # Jump\n",
    "            else:\n",
    "                cursor += 1 # Sync search\n",
    "        except:\n",
    "            cursor += 1\n",
    "            \n",
    "    if valid_rows:\n",
    "        pd.DataFrame(valid_rows).to_csv(out_csv_path, index=False)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# ==========================================\n",
    "# 3. ENRICHMENT (With Physics Time)\n",
    "# ==========================================\n",
    "def enrich_data(raw_csv, meta):\n",
    "    df = pd.read_csv(raw_csv)\n",
    "    if df.empty: return False\n",
    "    \n",
    "    # 1. Deduplicate\n",
    "    df.drop_duplicates(subset=['Latitude', 'Longitude'], keep='first', inplace=True)\n",
    "    \n",
    "    # 2. Physics Metrics\n",
    "    transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", \"EPSG:25832\", always_xy=True)\n",
    "    xx, yy = transformer.transform(df['Longitude'].values, df['Latitude'].values)\n",
    "    df['Step_Dist_M'] = np.sqrt(np.diff(xx, prepend=xx[0])**2 + np.diff(yy, prepend=yy[0])**2)\n",
    "    \n",
    "    # 3. Speed\n",
    "    # Try to find a speed column\n",
    "    df['Calc_Speed'] = 2.0\n",
    "    \n",
    "    speed_col = None\n",
    "    if 'Raw_Speed' in df.columns: speed_col = 'Raw_Speed'\n",
    "    elif 'Speed' in df.columns: speed_col = 'Speed'\n",
    "    elif 'DDI_018D' in df.columns: speed_col = 'DDI_018D'\n",
    "    \n",
    "    if speed_col:\n",
    "        # Scale: usually mm/s -> m/s\n",
    "        s = pd.to_numeric(df[speed_col], errors='coerce').fillna(0) * 0.001\n",
    "        mask_valid = (s > 0.1) & (s < 25.0)\n",
    "        df.loc[mask_valid, 'Calc_Speed'] = s[mask_valid]\n",
    "\n",
    "    # 4. Time Reconstruction\n",
    "    df['Step_Time_S'] = df['Step_Dist_M'] / df['Calc_Speed'].clip(lower=0.1)\n",
    "    df['Step_Time_S'] = df['Step_Time_S'].clip(upper=15.0)\n",
    "    \n",
    "    try:\n",
    "        anchor = datetime.fromisoformat(meta['StartTime'].replace('Z', '+00:00'))\n",
    "    except:\n",
    "        anchor = datetime(int(meta['Year']), 1, 1, 12, 0, 0)\n",
    "    df['Datetime'] = anchor + pd.to_timedelta(df['Step_Time_S'].cumsum(), unit='s')\n",
    "    \n",
    "    # 5. Yield\n",
    "    mass_col = None\n",
    "    if 'Raw_Yield_Mass' in df.columns: mass_col = 'Raw_Yield_Mass'\n",
    "    elif 'Yield_Mass' in df.columns: mass_col = 'Yield_Mass'\n",
    "    \n",
    "    df['Mass_Flow_Kg_S'] = 0\n",
    "    if mass_col:\n",
    "        # Scale: mg/s -> kg/s\n",
    "        m = pd.to_numeric(df[mass_col], errors='coerce').fillna(0)\n",
    "        df['Mass_Flow_Kg_S'] = m / 1_000_000.0\n",
    "        \n",
    "    HEADER_WIDTH = 9.0\n",
    "    df['Yield_T_Ha'] = (df['Mass_Flow_Kg_S'] * 10.0) / (df['Calc_Speed'] * HEADER_WIDTH)\n",
    "    df['Yield_T_Ha'] = df['Yield_T_Ha'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # 6. Save (The Fix)\n",
    "    def clean(s): return str(s).strip().replace(' ', '_').replace('/', '-').replace('\\\\', '-').replace(':', '')\n",
    "    year = int(meta['Year'])\n",
    "    safe_field = clean(meta['FieldName'])\n",
    "    safe_crop = clean(meta['Crop'])\n",
    "    tlg_id = clean(meta['LogFilename'].replace('.bin',''))\n",
    "    \n",
    "    if 'import' in safe_field.lower(): safe_field = tlg_id\n",
    "        \n",
    "    year_dir = os.path.join(ENRICHED_FOLDER, str(year))\n",
    "    if not os.path.exists(year_dir): os.makedirs(year_dir)\n",
    "    \n",
    "    out_name = f\"{year}_{safe_field}_{safe_crop}.csv\"\n",
    "    if os.path.exists(os.path.join(year_dir, out_name)):\n",
    "        out_name = f\"{year}_{safe_field}_{safe_crop}_{tlg_id}.csv\"\n",
    "        \n",
    "    # Save useful columns\n",
    "    out_cols = ['Datetime', 'Latitude', 'Longitude', 'Yield_T_Ha', 'Calc_Speed', 'Mass_Flow_Kg_S']\n",
    "    for c in ['Raw_Yield_Vol', 'Raw_Moisture', 'Moisture']:\n",
    "        if c in df.columns: out_cols.append(c)\n",
    "        \n",
    "    df[out_cols].to_csv(os.path.join(year_dir, out_name), index=False)\n",
    "    return True\n",
    "\n",
    "# ==========================================\n",
    "# EXECUTION\n",
    "# ==========================================\n",
    "def run_pipeline():\n",
    "    print(\"--- 1. BUILDING INDEX ---\")\n",
    "    tree = ET.parse(os.path.join(DATA_FOLDER, 'TASKDATA.XML'))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    products = {p.attrib.get('A'): p.attrib.get('B') for p in root.findall(\".//PDT\")}\n",
    "    fields = {f.attrib.get('A'): f.attrib.get('C') for f in root.findall(\".//PFD\")}\n",
    "    \n",
    "    tasks = []\n",
    "    for tsk in root.findall(\".//TSK\"):\n",
    "        tlg = tsk.find(\"TLG\")\n",
    "        if tlg is None: continue\n",
    "        log = tlg.attrib.get('A') + '.bin'\n",
    "        \n",
    "        # Parse DDIs\n",
    "        ddis = []\n",
    "        tim = tsk.find(\"TIM\")\n",
    "        if tim is not None:\n",
    "             s_time = tim.attrib.get('A')\n",
    "             for dlv in tim.findall(\"DLV\"): ddis.append(dlv.attrib.get('A'))\n",
    "        else: s_time = \"\"\n",
    "\n",
    "        # Sidecar check for DDIs\n",
    "        sidecar = os.path.join(DATA_FOLDER, log.replace('.bin','.xml'))\n",
    "        if os.path.exists(sidecar):\n",
    "            try: \n",
    "                st = ET.parse(sidecar)\n",
    "                ddis = [d.attrib.get('A') for d in st.findall(\".//DLV\")]\n",
    "            except: pass\n",
    "            \n",
    "        crop = products.get(tsk.find(\"PAN\").attrib.get('A'), \"Unknown\") if tsk.find(\"PAN\") is not None else \"Unknown\"\n",
    "        f_name = fields.get(tsk.attrib.get('E'), \"Unknown\")\n",
    "        \n",
    "        year = 2024\n",
    "        try: year = datetime.fromisoformat(s_time.replace('Z','')).year\n",
    "        except: pass\n",
    "        \n",
    "        tasks.append({'LogFilename': log, 'Year': year, 'Crop': crop, 'FieldName': f_name, 'StartTime': s_time, 'DDI_List': json.dumps(ddis)})\n",
    "        \n",
    "    df_index = pd.DataFrame(tasks)\n",
    "    df_index.to_csv(os.path.join(INTERIM_FOLDER, 'task_index.csv'), index=False)\n",
    "    print(f\"Found {len(df_index)} tasks.\")\n",
    "\n",
    "    print(\"\\n--- 2. PROCESSING ---\")\n",
    "    count = 0\n",
    "    for idx, row in df_index.iterrows():\n",
    "        bin_file = os.path.join(DATA_FOLDER, row['LogFilename'])\n",
    "        raw_csv = os.path.join(INTERIM_FOLDER, row['LogFilename'].replace('.bin', '.csv'))\n",
    "        ddi_list = json.loads(row['DDI_List'])\n",
    "        \n",
    "        # print(f\"Processing {row['LogFilename']}...\", end='\\r')\n",
    "        if extract_file(bin_file, raw_csv, ddi_list):\n",
    "            if enrich_data(raw_csv, row):\n",
    "                count += 1\n",
    "                \n",
    "    print(f\"\\nDone. {count} files enriched in '{ENRICHED_FOLDER}'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a3596-3670-432c-8eed-6dd2acc68158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
