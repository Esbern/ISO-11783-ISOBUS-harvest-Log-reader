{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0390063-6d5f-4ed1-961c-6646859aca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b1b5e02-4d35-4432-997a-b514707a80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soecifu the taskdata folder\n",
    "\n",
    "data_folder = r'C:/dev/agri_analysis/data/taskdata' \n",
    "out_folder = r'C:/dev/agri_analysis/data/taskdata_out2'\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "# Buffer to add around the farm boundaries (in degrees)\n",
    "# 0.02 deg is approx 2km. This accounts for headlands/approach roads.\n",
    "GEO_BUFFER = 0.02 \n",
    "\n",
    "# DEFINE YOUR LOCAL PROJECTION HERE for spped and mapping purposes\n",
    "# EPSG:25832 = ETRS89 / UTM zone 32N (Standard for Denmark)\n",
    "# EPSG:32632 = WGS 84 / UTM zone 32N (International standard)\n",
    "target_crs = \"EPSG:25832\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e2a81-ca46-4243-a785-ac3fa4cba27b",
   "metadata": {},
   "source": [
    "## XML task file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e964cab8-18f1-4096-abd9-3498d463b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_isobus_taskdata(data_folder):\n",
    "    # 1. Find TASKDATA.XML\n",
    "    taskdata_path = None\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.upper() == 'TASKDATA.XML':\n",
    "            taskdata_path = os.path.join(data_folder, file)\n",
    "            break\n",
    "            \n",
    "    if taskdata_path is None:\n",
    "        print(\"Error: TASKDATA.XML not found.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        tree = ET.parse(taskdata_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 1. Map Products (PDT) ---\n",
    "    products = {}\n",
    "    for pdt in root.findall(\".//PDT\"):\n",
    "        products[pdt.attrib.get('A')] = pdt.attrib.get('B')\n",
    "\n",
    "    # --- 2. Store Field Geometries (PFD) ---\n",
    "    field_shapes = {}   # PFD_ID -> Coordinates\n",
    "    field_names_map = {} # PFD_ID -> Name\n",
    "\n",
    "    for pfd in root.findall(\".//PFD\"):\n",
    "        pfd_id = pfd.attrib.get('A')\n",
    "        field_names_map[pfd_id] = pfd.attrib.get('C')\n",
    "        \n",
    "        # Extract Geometry (Polygon)\n",
    "        # We take the first valid polygon found\n",
    "        for pln in pfd.findall(\"PLN\"):\n",
    "            for lsg in pln.findall(\"LSG\"):\n",
    "                coordinates = []\n",
    "                for pnt in lsg.findall(\"PNT\"):\n",
    "                    try:\n",
    "                        # XML Attributes are strings, convert to float\n",
    "                        lat = float(pnt.attrib.get('C'))\n",
    "                        lon = float(pnt.attrib.get('D'))\n",
    "                        coordinates.append([lon, lat])\n",
    "                    except: continue\n",
    "                \n",
    "                if len(coordinates) > 2:\n",
    "                    # Close the polygon loop\n",
    "                    if coordinates[0] != coordinates[-1]:\n",
    "                        coordinates.append(coordinates[0])\n",
    "                    field_shapes[pfd_id] = coordinates\n",
    "                    break # Stop after first valid shape\n",
    "\n",
    "    # --- 3. Build Tasks & Generate GeoJSON ---\n",
    "    tasks_list = []\n",
    "    geojson_features = []\n",
    "\n",
    "    for tsk in root.findall(\".//TSK\"):\n",
    "        task_id = tsk.attrib.get('A')\n",
    "        field_ref = tsk.attrib.get('E')\n",
    "        \n",
    "        # Get Attributes\n",
    "        field_name = field_names_map.get(field_ref, f\"Unknown ({field_ref})\")\n",
    "        \n",
    "        # Get Log File\n",
    "        tlg = tsk.find(\"TLG\")\n",
    "        log_filename = tlg.attrib.get('A') if tlg is not None else None\n",
    "\n",
    "        # Get Crop\n",
    "        crop_name = \"Unknown\"\n",
    "        pan = tsk.find(\"PAN\")\n",
    "        if pan is not None:\n",
    "            pdt_ref = pan.attrib.get('A')\n",
    "            crop_name = products.get(pdt_ref, pdt_ref)\n",
    "            \n",
    "        # --- NEW: GET YEAR ---\n",
    "        # Find all TIM tags and get the earliest start time\n",
    "        start_times = []\n",
    "        for tim in tsk.findall(\"TIM\"):\n",
    "            start_str = tim.attrib.get('A') # Format: 2023-08-16T11:34:57...\n",
    "            if start_str:\n",
    "                try:\n",
    "                    # Parse ISO format (handle timezone if present, simplistically here)\n",
    "                    dt = datetime.fromisoformat(start_str.replace('Z', '+00:00'))\n",
    "                    start_times.append(dt)\n",
    "                except: pass\n",
    "        \n",
    "        task_year = start_times[0].year if start_times else None\n",
    "\n",
    "        # -- Build DataFrame Entry --\n",
    "        if log_filename: # Only save interesting tasks with logs\n",
    "            tasks_list.append({\n",
    "                'TaskID': task_id,\n",
    "                'Year': task_year,\n",
    "                'Crop': crop_name,\n",
    "                'FieldName': field_name,\n",
    "                'FieldID': field_ref,\n",
    "                'LogFilename': log_filename\n",
    "            })\n",
    "\n",
    "            # -- Build GeoJSON Feature --\n",
    "            if field_ref in field_shapes:\n",
    "                feature = {\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"properties\": {\n",
    "                        \"TaskID\": task_id,\n",
    "                        \"Year\": int(task_year) if task_year else None,\n",
    "                        \"FieldName\": field_name,\n",
    "                        \"Crop\": crop_name,\n",
    "                        \"LogFilename\": log_filename \n",
    "                    },\n",
    "                    \"geometry\": {\n",
    "                        \"type\": \"Polygon\",\n",
    "                        \"coordinates\": [field_shapes[field_ref]]\n",
    "                    }\n",
    "                }\n",
    "                geojson_features.append(feature)\n",
    "\n",
    "    # --- Output ---\n",
    "    geojson_output = {\n",
    "        \"type\": \"FeatureCollection\", \n",
    "        \"features\": geojson_features\n",
    "    }\n",
    "    df_tasks = pd.DataFrame(tasks_list)\n",
    "    \n",
    "    return geojson_output, df_tasks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b79b52-5df0-4395-9cc8-050ebdbdea9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. Index saved to C:/dev/agri_analysis/data/taskdata_out2\\task_index.csv\n",
      "Sample:\n",
      "   Year              Crop      FieldName\n",
      "0  2024  RA - Raps / rybs  021-0, Monica\n",
      "1  2024  RA - Raps / rybs  021-0, Monica\n",
      "2  2025        HV - Hvede         Import\n",
      "3  2025        HV - Hvede         Import\n",
      "4  2025       VB - VÃ¥rbyg         Import\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTION ---\n",
    "geojson, df_tasks = parse_isobus_taskdata(data_folder)\n",
    "\n",
    "if df_tasks is not None:\n",
    "    # Save CSV\n",
    "    csv_path = os.path.join(out_folder, 'task_index.csv')\n",
    "    df_tasks.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Save GeoJSON\n",
    "    geo_path = os.path.join(out_folder, 'harvest_tasks.geojson')\n",
    "    with open(geo_path, 'w') as f:\n",
    "        json.dump(geojson, f)\n",
    "        \n",
    "    print(f\"Success. Index saved to {csv_path}\")\n",
    "    print(f\"Sample:\\n{df_tasks[['Year', 'Crop', 'FieldName']].head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb3fbe-f7d5-47db-b8db-bdfcba69f27e",
   "metadata": {},
   "source": [
    "## Binary file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82f986c2-6f20-466e-8636-2e5a37c24554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER: PACKET CLASSIFIER ---\n",
    "def classify_packet(gap_size):\n",
    "    \"\"\"\n",
    "    Determines the machine state based on the size of the proprietary header\n",
    "    preceding the data record.\n",
    "    \"\"\"\n",
    "    if gap_size == 25:\n",
    "        return \"Harvest (Mode A)\"\n",
    "    elif gap_size == 10:\n",
    "        return \"Transport (Mode B)\"\n",
    "    elif gap_size == 0:\n",
    "        return \"Contiguous\" # Rare, usually start of file\n",
    "    else:\n",
    "        return f\"Transition ({gap_size}b)\"\n",
    "\n",
    "# --- HELPER: DYNAMIC BOUNDING BOX ---\n",
    "def get_farm_bounding_box(taskdata_path):\n",
    "    print(f\"Reading Farm Geometry from: {taskdata_path}\")\n",
    "    try:\n",
    "        tree = ET.parse(taskdata_path)\n",
    "        root = tree.getroot()\n",
    "        lats, lons = [], []\n",
    "        for pnt in root.findall(\".//PFD//PNT\"):\n",
    "            try:\n",
    "                lats.append(float(pnt.attrib.get('C')))\n",
    "                lons.append(float(pnt.attrib.get('D')))\n",
    "            except: continue\n",
    "            \n",
    "        if not lats: return 54.5, 58.0, 8.0, 15.0\n",
    "        return min(lats)-GEO_BUFFER, max(lats)+GEO_BUFFER, min(lons)-GEO_BUFFER, max(lons)+GEO_BUFFER\n",
    "    except: return 54.5, 58.0, 8.0, 15.0\n",
    "\n",
    "# --- HELPER: METADATA & DEFS ---\n",
    "def load_metadata(taskdata_path):\n",
    "    try:\n",
    "        tree = ET.parse(taskdata_path)\n",
    "        root = tree.getroot()\n",
    "        pdt_map = {pdt.attrib.get('A'): pdt.attrib.get('B') for pdt in root.findall(\".//PDT\")}\n",
    "        meta_map = {}\n",
    "        for tsk in root.findall(\".//TSK\"):\n",
    "            tlg = tsk.find(\".//TLG\")\n",
    "            pan = tsk.find(\".//PAN\")\n",
    "            tim = tsk.find(\".//TIM\")\n",
    "            if tlg is not None:\n",
    "                log_id = tlg.attrib.get('A')\n",
    "                crop = pdt_map.get(pan.attrib.get('A'), 'Unknown') if pan is not None else 'Unknown'\n",
    "                start = tim.attrib.get('A') if tim is not None else None\n",
    "                meta_map[log_id] = {'Crop': crop, 'Start_Time': start}\n",
    "                meta_map[log_id + '.bin'] = {'Crop': crop, 'Start_Time': start}\n",
    "        return meta_map\n",
    "    except: return {}\n",
    "\n",
    "def get_dlv_defs(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        return [elem.attrib.get('A') for elem in tree.findall(\".//DLV\")]\n",
    "    except: return []\n",
    "\n",
    "# --- CORE: ENRICHED FORENSIC SCANNER (Corrected: Unsigned 'I') ---\n",
    "def forensic_scan(bin_path, definitions, metadata, bounds):\n",
    "    min_lat, max_lat, min_lon, max_lon = bounds\n",
    "    filename = os.path.basename(bin_path)\n",
    "    meta = metadata.get(filename, {'Crop': 'Unknown', 'Start_Time': None})\n",
    "    \n",
    "    with open(bin_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    total_len = len(content)\n",
    "    cursor = 0\n",
    "    valid_rows = []\n",
    "    last_valid_end = 0\n",
    "    \n",
    "    # Map DDI IDs to human names\n",
    "    ddi_lookup = {'0054': 'Yield_Mass', '0095': 'Yield_Vol', '018D': 'Speed', '0063': 'Moisture'}\n",
    "\n",
    "    while cursor < total_len - 50:\n",
    "        match_found = False\n",
    "        bytes_consumed = 0\n",
    "        \n",
    "        try:\n",
    "            # Header: Time (Unsigned Long 'L'), Lat (Signed 'i'), Lon (Signed 'i')\n",
    "            # Lat/Lon must remain Signed because coordinates can be negative\n",
    "            time_ms, lat_raw, lon_raw = struct.unpack('<Lii', content[cursor:cursor+12])\n",
    "            lat = lat_raw * 1e-7\n",
    "            lon = lon_raw * 1e-7\n",
    "            \n",
    "            if (min_lat < lat < max_lat) and (min_lon < lon < max_lon):\n",
    "                count = len(definitions)\n",
    "                payload_len = count * 4\n",
    "                \n",
    "                if cursor + 12 + payload_len <= total_len:\n",
    "                    # CRITICAL FIX: Use 'I' (Unsigned Int) for sensor payload\n",
    "                    # This prevents 0x80000000 from becoming a negative number\n",
    "                    values = struct.unpack(f\"{count}I\", content[cursor+12 : cursor+12+payload_len])\n",
    "                    \n",
    "                    row_data = {'Time_ms': time_ms, 'Latitude': lat, 'Longitude': lon}\n",
    "                    for i, val in enumerate(values):\n",
    "                        ddi = definitions[i]\n",
    "                        row_data[ddi_lookup.get(ddi, f'DDI_{ddi}')] = val\n",
    "                    \n",
    "                    bytes_consumed = 12 + payload_len\n",
    "                    match_found = True\n",
    "        except: pass\n",
    "\n",
    "        if match_found:\n",
    "            gap_size = cursor - last_valid_end\n",
    "            \n",
    "            row_data['File'] = filename\n",
    "            row_data['Crop'] = meta['Crop']\n",
    "            row_data['Start_Time_Str'] = meta['Start_Time']\n",
    "            row_data['Gap_Bytes'] = gap_size\n",
    "            row_data['Packet_Type'] = classify_packet(gap_size)\n",
    "\n",
    "            valid_rows.append(row_data)\n",
    "\n",
    "            cursor += bytes_consumed\n",
    "            last_valid_end = cursor\n",
    "        else:\n",
    "            cursor += 1\n",
    "\n",
    "    return pd.DataFrame(valid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7f840d-1e69-4dd2-874d-9da0c44c24e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Farm Geometry from: ./data/taskdata\\TASKDATA.XML\n",
      "Starting Enriched Forensic Scan...\n",
      "Processing Metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\holmes\\AppData\\Local\\Temp\\ipykernel_20672\\379185482.py:47: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  master = master.groupby('File', group_keys=False).apply(reconstruct_continuous_time)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Reconstructing GPS Speed...\n",
      "\n",
      "SUCCESS.\n",
      "Dataset saved to: ./data/taskdata_out\\UNIVERSAL_ENRICHED_DATASET.csv\n",
      "Speed Window: 5 samples\n",
      "Sensor Speed Scaled (Factor 1000): Max 2145386.50 m/s\n",
      "Moisture Scaled (Factor 100): Max 21472299.87 %\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTION ---\n",
    "taskdata_xml = os.path.join(data_folder, 'TASKDATA.XML')\n",
    "farm_bounds = get_farm_bounding_box(taskdata_xml)\n",
    "metadata_map = load_metadata(taskdata_xml)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "print(\"Starting Enriched Forensic Scan...\")\n",
    "\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.upper().startswith(\"TLG\") and filename.upper().endswith(\".BIN\"):\n",
    "        xml_full = os.path.join(data_folder, filename.rsplit('.', 1)[0] + \".xml\")\n",
    "        bin_full = os.path.join(data_folder, filename)\n",
    "        \n",
    "        if os.path.exists(xml_full):\n",
    "            \n",
    "            defs = get_dlv_defs(xml_full)\n",
    "            \n",
    "            df = forensic_scan(bin_full, defs, metadata_map, farm_bounds)\n",
    " \n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "\n",
    "\n",
    "# --- POST-PROCESSING (Tuned for Sparse Data) ---\n",
    "if all_data:\n",
    "    print(\"Processing Metrics...\")\n",
    "    \n",
    "    # 1. Concatenate strictly in order\n",
    "    master = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates to prevent time errors\n",
    "    master.drop_duplicates(subset=['File', 'Time_ms', 'Latitude', 'Longitude'], keep='first', inplace=True)\n",
    "\n",
    "    # 2. Time Reconstruction\n",
    "    def reconstruct_continuous_time(group):\n",
    "        diffs = group['Time_ms'].diff().fillna(0)\n",
    "        wrap_mask = diffs < -1000 \n",
    "        raw_max = group['Time_ms'].max()\n",
    "        if 59000 < raw_max < 61000: cycle_size = 60000 \n",
    "        elif 65000 < raw_max < 66000: cycle_size = 65536\n",
    "        else: cycle_size = raw_max \n",
    "        group['Wrap_Offset'] = wrap_mask.cumsum() * cycle_size\n",
    "        group['Time_Continuous'] = group['Time_ms'] + group['Wrap_Offset']\n",
    "        return group\n",
    "\n",
    "    master = master.groupby('File', group_keys=False).apply(reconstruct_continuous_time)\n",
    "    master['Task_Start'] = pd.to_datetime(master['Start_Time_Str'])\n",
    "    master['Datetime'] = master['Task_Start'] + pd.to_timedelta(master['Time_Continuous'], unit='ms')\n",
    "\n",
    "    # 3. Sensor Cleaning & Scaling (Corrected for your Units)\n",
    "    def clean_and_scale(series, scale_factor=1.0):\n",
    "        s = pd.to_numeric(series, errors='coerce').fillna(0)\n",
    "        # Fix Unsigned Wrap & Clamp Negative Noise\n",
    "        s = np.where(s > 2147483647, 0, s)\n",
    "        s = np.where(s < 0, 0, s)\n",
    "        return s / scale_factor\n",
    "\n",
    "    # SPEED: Raw is ~1000. Unit is mm/s. Divide by 1000 to get m/s.\n",
    "    if 'Speed' in master.columns:\n",
    "        master['Speed'] = clean_and_scale(master['Speed'], scale_factor=1000.0)\n",
    "\n",
    "    # MOISTURE: Raw is ~990. Unit is 0.01%. Divide by 100 to get %.\n",
    "    if 'Moisture' in master.columns:\n",
    "        master['Moisture'] = clean_and_scale(master['Moisture'], scale_factor=100.0)\n",
    "\n",
    "    master['Raw_Mass'] = clean_and_scale(master['Yield_Mass'], scale_factor=1.0)\n",
    "    master['Raw_Vol']  = clean_and_scale(master['Yield_Vol'], scale_factor=1.0)\n",
    "    master['Status_Code'] = master['Raw_Mass'].astype(int) & 0xFF\n",
    "    master['Density_kg_L'] = np.where(master['Raw_Vol'] > 1000, (master['Raw_Mass'] / master['Raw_Vol']), 0)\n",
    "\n",
    "    # 4. GPS SPEED (Window Fixed for 20m Steps)\n",
    "    print(\"  -> Reconstructing GPS Speed...\")\n",
    "    import pyproj\n",
    "    transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", \"EPSG:25832\", always_xy=True)\n",
    "    xx, yy = transformer.transform(master['Longitude'].values, master['Latitude'].values)\n",
    "    master['UTM_Easting'] = xx\n",
    "    master['UTM_Northing'] = yy\n",
    "\n",
    "    # UPDATED WINDOW: 5 samples (~100m) is enough since each step is huge (20m)\n",
    "    SPEED_WINDOW = 5 \n",
    "    \n",
    "    grp = master.groupby('File')\n",
    "    \n",
    "    dx = grp['UTM_Easting'].diff(SPEED_WINDOW)\n",
    "    dy = grp['UTM_Northing'].diff(SPEED_WINDOW)\n",
    "    dist_m = np.sqrt(dx**2 + dy**2)\n",
    "    dt_s = grp['Time_Continuous'].diff(SPEED_WINDOW) / 1000.0\n",
    "    dt_s = dt_s.where(dt_s > 0.1, np.nan) \n",
    "    \n",
    "    master['GPS_Speed'] = dist_m / dt_s\n",
    "    master['GPS_Speed'] = master['GPS_Speed'].bfill().fillna(0).clip(0, 25)\n",
    "\n",
    "    # 5. Yield Calc (Using SENSOR SPEED if available, else GPS)\n",
    "    # Since your GPS points are sparse (20m gaps), Sensor Speed (mm/s) is likely more accurate for Yield.\n",
    "    HEADER_WIDTH = 9.0 \n",
    "    master['Mass_kg_s'] = master['Raw_Mass'] / 1_000_000\n",
    "    \n",
    "    # Use Sensor Speed if it's valid (>0.1), otherwise fallback to GPS\n",
    "    # This prevents the \"20m jump\" from messing up the yield\n",
    "    use_speed = np.where(master['Speed'] > 0.1, master['Speed'], master['GPS_Speed'])\n",
    "    \n",
    "    master['Yield_T_Ha'] = (master['Mass_kg_s'] * 10.0) / (use_speed * HEADER_WIDTH)\n",
    "    master['Yield_T_Ha'] = master['Yield_T_Ha'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # --- SAVING ---\n",
    "    out_csv = os.path.join(out_folder, 'UNIVERSAL_ENRICHED_DATASET.csv')\n",
    "    \n",
    "    cols = [\n",
    "        'Datetime', 'Latitude', 'Longitude', \n",
    "        'UTM_Easting', 'UTM_Northing',\n",
    "        'File', 'Crop', 'Packet_Type', 'Gap_Bytes',   \n",
    "        'Yield_T_Ha', 'Density_kg_L', \n",
    "        'GPS_Speed', 'Speed', 'Moisture',\n",
    "        'Status_Code', 'Raw_Mass', 'Raw_Vol'\n",
    "    ]\n",
    "    \n",
    "    valid_cols = [c for c in cols if c in master.columns]\n",
    "    master[valid_cols].to_csv(out_csv, index=False)\n",
    "    \n",
    "    print(f\"\\nSUCCESS.\")\n",
    "    print(f\"Dataset saved to: {out_csv}\")\n",
    "    print(f\"Speed Window: {SPEED_WINDOW} samples\")\n",
    "    if 'Speed' in master.columns:\n",
    "        print(f\"Sensor Speed Scaled (Factor 1000): Max {master['Speed'].max():.2f} m/s\")\n",
    "    if 'Moisture' in master.columns:\n",
    "        print(f\"Moisture Scaled (Factor 100): Max {master['Moisture'].max():.2f} %\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3050d7-bb91-468f-b9f5-0c22b817878e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
