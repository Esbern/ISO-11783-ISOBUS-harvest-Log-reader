{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90529240-0107-4e65-9c73-6a9071f4f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec259c9-b832-4a62-84d3-baa54fb52a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "DATA_FOLDER = r'./data/TASKDATA'\n",
    "INTERIM_FOLDER  = r'./data/taskdata_out2'\n",
    "ENRICHED_FOLDER = r'./data/ENRICHED'\n",
    "OUT_ESPG = \"EPSG:25832\"\n",
    "# Safety Buffer: Add ~1km (0.01 deg) around the field to catch headland turns\n",
    "GEO_BUFFER = 0.01 \n",
    "BBOX_DEFAULT = (54.0, 58.0, 8.0, 16.0) # Denmark\n",
    "if not os.path.exists(INTERIM_FOLDER):\n",
    "    os.makedirs(INTERIM_FOLDER)\n",
    "if not os.path.exists(ENRICHED_FOLDER):\n",
    "    os.makedirs(ENRICHED_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ba9a7a-e76d-4e82-a5c0-ad5f0026041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. METADATA PARSER (Generates Index + Bounding Box)\n",
    "# ==========================================\n",
    "def parse_isobus_taskdata(data_folder):\n",
    "    print(f\"Scanning {data_folder} for TASKDATA.XML...\")\n",
    "    \n",
    "    # Find XML\n",
    "    taskdata_path = None\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.upper() == 'TASKDATA.XML':\n",
    "            taskdata_path = os.path.join(data_folder, file)\n",
    "            break\n",
    "            \n",
    "    if taskdata_path is None:\n",
    "        print(\"Error: TASKDATA.XML not found.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        tree = ET.parse(taskdata_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # --- A. Map Products (PDT) ---\n",
    "    products = {}\n",
    "    for pdt in root.findall(\".//PDT\"):\n",
    "        products[pdt.attrib.get('A')] = pdt.attrib.get('B')\n",
    "\n",
    "    # --- B. Store Field Geometries & Calculate Bounds ---\n",
    "    field_shapes = {}\n",
    "    field_bounds = {} # PFD_ID -> (min_lat, max_lat, min_lon, max_lon)\n",
    "    field_names = {}\n",
    "\n",
    "    for pfd in root.findall(\".//PFD\"):\n",
    "        pfd_id = pfd.attrib.get('A')\n",
    "        field_names[pfd_id] = pfd.attrib.get('C')\n",
    "        \n",
    "        for pln in pfd.findall(\"PLN\"):\n",
    "            for lsg in pln.findall(\"LSG\"):\n",
    "                lats, lons = [], []\n",
    "                coords = []\n",
    "                for pnt in lsg.findall(\"PNT\"):\n",
    "                    try:\n",
    "                        lat = float(pnt.attrib.get('C'))\n",
    "                        lon = float(pnt.attrib.get('D'))\n",
    "                        lats.append(lat)\n",
    "                        lons.append(lon)\n",
    "                        coords.append([lon, lat])\n",
    "                    except: continue\n",
    "                \n",
    "                if lats:\n",
    "                    # Save Polygon\n",
    "                    if coords[0] != coords[-1]: coords.append(coords[0])\n",
    "                    field_shapes[pfd_id] = coords\n",
    "                    \n",
    "                    # Calculate Bounding Box (With Buffer)\n",
    "                    field_bounds[pfd_id] = (\n",
    "                        min(lats) - GEO_BUFFER, max(lats) + GEO_BUFFER,\n",
    "                        min(lons) - GEO_BUFFER, max(lons) + GEO_BUFFER\n",
    "                    )\n",
    "                    break \n",
    "\n",
    "    # --- C. Build Task Index ---\n",
    "    tasks_list = []\n",
    "    geojson_features = []\n",
    "\n",
    "    for tsk in root.findall(\".//TSK\"):\n",
    "        task_id = tsk.attrib.get('A')\n",
    "        field_ref = tsk.attrib.get('E')\n",
    "        \n",
    "        field_name = field_names.get(field_ref, f\"Unknown ({field_ref})\")\n",
    "        \n",
    "        # Get Log File\n",
    "        tlg = tsk.find(\"TLG\")\n",
    "        log_filename = tlg.attrib.get('A') if tlg is not None else None\n",
    "\n",
    "        # Get Crop\n",
    "        crop_name = \"Unknown\"\n",
    "        pan = tsk.find(\"PAN\")\n",
    "        if pan is not None:\n",
    "            pdt_ref = pan.attrib.get('A')\n",
    "            crop_name = products.get(pdt_ref, pdt_ref)\n",
    "            \n",
    "        # Get Year\n",
    "        start_times = []\n",
    "        for tim in tsk.findall(\"TIM\"):\n",
    "            start_str = tim.attrib.get('A') \n",
    "            if start_str:\n",
    "                try:\n",
    "                    dt = datetime.fromisoformat(start_str.replace('Z', '+00:00'))\n",
    "                    start_times.append(dt)\n",
    "                except: pass\n",
    "        task_year = start_times[0].year if start_times else None\n",
    "\n",
    "        if log_filename:\n",
    "            # Get Bounds for this field (Default to Denmark if unknown)\n",
    "            # Default: 54-58N, 8-16E\n",
    "            bounds = field_bounds.get(field_ref, BBOX_DEFAULT)\n",
    "            \n",
    "            tasks_list.append({\n",
    "                'TaskID': task_id,\n",
    "                'Year': task_year,\n",
    "                'Crop': crop_name,\n",
    "                'FieldName': field_name,\n",
    "                'LogFilename': log_filename + '.bin' if not log_filename.endswith('.bin') else log_filename,\n",
    "                'MinLat': bounds[0], 'MaxLat': bounds[1],\n",
    "                'MinLon': bounds[2], 'MaxLon': bounds[3]\n",
    "            })\n",
    "\n",
    "            # GeoJSON\n",
    "            if field_ref in field_shapes:\n",
    "                geojson_features.append({\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"properties\": { \"TaskID\": task_id, \"FieldName\": field_name, \"Crop\": crop_name },\n",
    "                    \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [field_shapes[field_ref]] }\n",
    "                })\n",
    "\n",
    "    return { \"type\": \"FeatureCollection\", \"features\": geojson_features }, pd.DataFrame(tasks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95881da-e02a-4387-b6a1-439681fb1903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. RAW BINARY CONVERTER (Guided by BBox)\n",
    "# ==========================================\n",
    "def convert_bin_to_csv(bin_path, out_csv_path, bounds):\n",
    "    \"\"\"\n",
    "    Reads binary, checks Geo-Bounds, outputs Raw Unsigned Values.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(bin_path): return False\n",
    "    \n",
    "    min_lat, max_lat, min_lon, max_lon = bounds\n",
    "    \n",
    "    with open(bin_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    total_len = len(content)\n",
    "    cursor = 0\n",
    "    valid_rows = []\n",
    "    last_valid_end = 0\n",
    "    \n",
    "    while cursor < total_len - 30:\n",
    "        match_found = False\n",
    "        bytes_consumed = 0\n",
    "        \n",
    "        try:\n",
    "            # 1. Header (Time=Unsigned, Lat/Lon=Signed)\n",
    "            time_ms, lat_raw, lon_raw = struct.unpack('<Lii', content[cursor:cursor+12])\n",
    "            lat = lat_raw * 1e-7\n",
    "            lon = lon_raw * 1e-7\n",
    "            \n",
    "            # 2. Geo-Filter (Using XML Bounds)\n",
    "            if (min_lat < lat < max_lat) and (min_lon < lon < max_lon):\n",
    "                \n",
    "                # 3. Payload (Strictly Unsigned 'I')\n",
    "                # 4 sensors * 4 bytes = 16 bytes\n",
    "                s1, s2, s3, s4 = struct.unpack('4I', content[cursor+12 : cursor+28])\n",
    "                \n",
    "                match_found = True\n",
    "                bytes_consumed = 28\n",
    "                \n",
    "                # 4. Gap Logic\n",
    "                gap_size = cursor - last_valid_end\n",
    "                if gap_size == 0: pkt_type = \"Contiguous\"\n",
    "                elif gap_size == 10: pkt_type = \"Transport (10b)\"\n",
    "                elif gap_size == 25: pkt_type = \"Harvest (25b)\"\n",
    "                else: pkt_type = f\"Gap ({gap_size}b)\"\n",
    "                \n",
    "                valid_rows.append({\n",
    "                    'Time_ms': time_ms,\n",
    "                    'Latitude': lat,\n",
    "                    'Longitude': lon,\n",
    "                    'Yield_Mass': s1, # Raw Unsigned\n",
    "                    'Yield_Vol': s2,  # Raw Unsigned\n",
    "                    'Speed': s3,      # Raw Unsigned\n",
    "                    'Moisture': s4,   # Raw Unsigned\n",
    "                    'Gap_Bytes': gap_size,\n",
    "                    'Packet_Type': pkt_type\n",
    "                })\n",
    "                \n",
    "        except: pass\n",
    "            \n",
    "        if match_found:\n",
    "            last_valid_end = cursor + bytes_consumed\n",
    "            cursor += bytes_consumed\n",
    "        else:\n",
    "            cursor += 1\n",
    "            \n",
    "    if valid_rows:\n",
    "        pd.DataFrame(valid_rows).to_csv(out_csv_path, index=False)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866359c8-e6fc-456c-8e18-8e0bfc342d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: METADATA & BOUNDS ---\n",
      "Scanning C:/dev/agri_analysis/data/taskdata for TASKDATA.XML...\n",
      "Index created. Found 142 tasks.\n",
      "\n",
      "--- STAGE 2: BINARY EXTRACTION ---\n",
      "Processing TLG00142.bin...\n",
      "Done. Extracted 142 files to C:/dev/agri_analysis/data/taskdata_out2/\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. EXECUTION LOOP\n",
    "# ==========================================\n",
    "print(\"--- STAGE 1: METADATA & BOUNDS ---\")\n",
    "geojson, df_tasks = parse_isobus_taskdata(DATA_FOLDER)\n",
    "\n",
    "if df_tasks is not None:\n",
    "    # Save Master Index\n",
    "    df_tasks.to_csv(os.path.join(INTERIM_FOLDER, 'task_index.csv'), index=False)\n",
    "    \n",
    "    # Save GeoJSON\n",
    "    with open(os.path.join(INTERIM_FOLDER, 'harvest_tasks.geojson'), 'w') as f:\n",
    "        json.dump(geojson, f)\n",
    "        \n",
    "    print(f\"Index created. Found {len(df_tasks)} tasks.\")\n",
    "    \n",
    "    print(\"\\n--- STAGE 2: BINARY EXTRACTION ---\")\n",
    "    count = 0\n",
    "    for idx, row in df_tasks.iterrows():\n",
    "        bin_file = os.path.join(DATA_FOLDER, row['LogFilename'])\n",
    "        out_file = os.path.join(INTERIM_FOLDER, row['LogFilename'].replace('.bin', '.csv'))\n",
    "        \n",
    "        # Get bounds from index\n",
    "        bounds = (row['MinLat'], row['MaxLat'], row['MinLon'], row['MaxLon'])\n",
    "        \n",
    "        print(f\"Processing {row['LogFilename']}...\", end='\\r')\n",
    "        if convert_bin_to_csv(bin_file, out_file, bounds):\n",
    "            count += 1\n",
    "            \n",
    "    print(f\"\\nDone. Extracted {count} files to {INTERIM_FOLDER}/\")\n",
    "else:\n",
    "    print(\"Metadata parsing failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cbf06d9-367f-453c-ad50-5b69a23ca187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING ENRICHMENT ---\n",
      "Found 142 tasks. Processing...\n",
      "Processed: TLG00142.bin -> 2023_010-0, Møllemark_HV - HvedeajgræsHvedeseyg\n",
      "\n",
      "SUCCESS. 142 tasks enriched.\n",
      "Data stored in: C:/dev/agri_analysis/data/ENRICHED/\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. HELPER: GET ANCHOR TIMES FROM XML\n",
    "# ==========================================\n",
    "def load_anchor_times(xml_folder):\n",
    "    \"\"\"\n",
    "    Extracts the PRECISE start time from XML to fix the \"2003\" binary date.\n",
    "    \"\"\"\n",
    "    anchors = {} \n",
    "    xml_path = os.path.join(xml_folder, 'TASKDATA.XML')\n",
    "    \n",
    "    if os.path.exists(xml_path):\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            for tsk in tree.findall(\".//TSK\"):\n",
    "                tlg = tsk.find(\"TLG\")\n",
    "                tim = tsk.find(\"TIM\")\n",
    "                \n",
    "                if tlg is not None and tim is not None:\n",
    "                    filename = tlg.attrib.get('A') + '.bin'\n",
    "                    start_str = tim.attrib.get('A')\n",
    "                    try:\n",
    "                        # Parse \"2024-08-12T09:00:00\"\n",
    "                        dt = datetime.fromisoformat(start_str.replace('Z', '+00:00'))\n",
    "                        anchors[filename] = dt\n",
    "                    except: pass\n",
    "        except: print(\"Warning: Could not parse XML dates.\")\n",
    "    \n",
    "    return anchors\n",
    "\n",
    "# ==========================================\n",
    "# 2. ENRICHMENT PROCESSOR\n",
    "# ==========================================\n",
    "def enrich_file(raw_path, meta, anchor_time):\n",
    "    # 1. Load Raw Bits\n",
    "    df = pd.read_csv(raw_path)\n",
    "    if df.empty: return False\n",
    "\n",
    "    # 2. FIX TIME (Anchor + Offset)\n",
    "    # We ignore the absolute value of Time_ms (2003) and just use the relative ticks\n",
    "    t0 = df['Time_ms'].iloc[0]\n",
    "    df['Seconds_Elapsed'] = (df['Time_ms'] - t0) / 1000.0\n",
    "    \n",
    "    # Fallback if XML time is missing: Use Jan 1st of the Task Year\n",
    "    if anchor_time is None:\n",
    "        year = meta['Year'] if pd.notnull(meta['Year']) else 2024\n",
    "        anchor_time = datetime(int(year), 1, 1, 12, 0, 0)\n",
    "        \n",
    "    df['Datetime'] = anchor_time + pd.to_timedelta(df['Seconds_Elapsed'], unit='s')\n",
    "\n",
    "    # 3. CLEAN & SCALE SENSORS\n",
    "    # Helper: Clean unsigned noise (>2B) and scale\n",
    "    def clean(s, scale):\n",
    "        s = pd.to_numeric(s, errors='coerce').fillna(0)\n",
    "        s = np.where(s > 2000000000, 0, s) # Filter Error Flags\n",
    "        return s / scale\n",
    "\n",
    "    # Scale based on your findings\n",
    "    if 'Speed' in df.columns:\n",
    "        df['Speed'] = clean(df['Speed'], 1000.0) # mm/s -> m/s\n",
    "    if 'Moisture' in df.columns:\n",
    "        df['Moisture'] = clean(df['Moisture'], 100.0) # 0.01% -> %\n",
    "        \n",
    "    df['Raw_Mass'] = clean(df['Yield_Mass'], 1_000_000.0) # mg/s -> kg/s\n",
    "    df['Raw_Vol']  = clean(df['Yield_Vol'], 1000.0)       # ml/s -> L/s\n",
    "    \n",
    "    # 4. RECALCULATE GPS SPEED (Rolling Window)\n",
    "    # 20m steps -> Window 5 (100m) is appropriate\n",
    "    transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", OUT_ESPG, always_xy=True)\n",
    "    xx, yy = transformer.transform(df['Longitude'].values, df['Latitude'].values)\n",
    "    df['UTM_Easting'] = xx\n",
    "    df['UTM_Northing'] = yy\n",
    "    \n",
    "    WINDOW = 5\n",
    "    dx = df['UTM_Easting'].diff(WINDOW)\n",
    "    dy = df['UTM_Northing'].diff(WINDOW)\n",
    "    dist = np.sqrt(dx**2 + dy**2)\n",
    "    dt = df['Seconds_Elapsed'].diff(WINDOW)\n",
    "    \n",
    "    df['GPS_Speed'] = dist / dt\n",
    "    df['GPS_Speed'] = df['GPS_Speed'].bfill().fillna(0)\n",
    "\n",
    "    # 5. YIELD CALCULATION\n",
    "    # Use Sensor Speed if moving, else GPS Speed\n",
    "    use_speed = np.where(df['Speed'] > 0.1, df['Speed'], df['GPS_Speed'])\n",
    "    HEADER_WIDTH = 9.0 # Standard combine header width\n",
    "    \n",
    "    # Yield (t/ha) = (kg/s * 10) / (m/s * m)\n",
    "    df['Yield_T_Ha'] = (df['Raw_Mass'] * 10.0) / (use_speed * HEADER_WIDTH)\n",
    "    df['Yield_T_Ha'] = df['Yield_T_Ha'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # 6. SAVE (Year_FieldName_Crop.csv)\n",
    "    # Sanitize names for Windows filesystems\n",
    "    safe_field = str(meta['FieldName']).replace(' ', '_').replace('/', '-').replace(',', '').replace('\"', '')\n",
    "    safe_crop  = str(meta['Crop']).replace(' ', '_').replace('/', '-')\n",
    "    year = int(meta['Year'])\n",
    "    \n",
    "    # ---  Create Year Subfolder ---\n",
    "    year_folder = os.path.join(ENRICHED_FOLDER, str(year))\n",
    "    if not os.path.exists(year_folder):\n",
    "        os.makedirs(year_folder)\n",
    "    \n",
    "    out_name = f\"{year}_{safe_field}_{safe_crop}.csv\"\n",
    "    out_path = os.path.join(year_folder, out_name)\n",
    "    \n",
    "    # Append to existing if file exists (Merge split tasks)\n",
    "    write_mode = 'w'\n",
    "    write_header = True\n",
    "    if os.path.exists(out_path):\n",
    "        write_mode = 'a'\n",
    "        write_header = False\n",
    "        \n",
    "    cols = ['Datetime', 'Latitude', 'Longitude','UTM_Northing', 'UTM_Easting','Yield_T_Ha', 'GPS_Speed', 'Speed', 'Moisture', 'Packet_Type']\n",
    "    df[cols].to_csv(out_path, mode=write_mode, header=write_header, index=False)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ==========================================\n",
    "# 3. EXECUTION\n",
    "# ==========================================\n",
    "print(\"--- STARTING ENRICHMENT ---\")\n",
    "index_path = os.path.join(INTERIM_FOLDER, 'task_index.csv')\n",
    "\n",
    "if os.path.exists(index_path):\n",
    "    df_index = pd.read_csv(index_path)\n",
    "    anchors = load_anchor_times(DATA_FOLDER)\n",
    "    \n",
    "    print(f\"Found {len(df_index)} tasks. Processing...\")\n",
    "    \n",
    "    count = 0\n",
    "    for idx, row in df_index.iterrows():\n",
    "        raw_csv = row['LogFilename'].replace('.bin', '.csv')\n",
    "        raw_path = os.path.join(INTERIM_FOLDER, raw_csv)\n",
    "        \n",
    "        if os.path.exists(raw_path):\n",
    "            start_time = anchors.get(row['LogFilename'])\n",
    "            if enrich_file(raw_path, row, start_time):\n",
    "                count += 1\n",
    "                print(f\"Processed: {row['LogFilename']} -> {row['Year']}_{row['FieldName']}_{row['Crop']}\", end='\\r')\n",
    "                \n",
    "    print(f\"\\n\\nSUCCESS. {count} tasks enriched.\")\n",
    "    print(f\"Data stored in: {ENRICHED_FOLDER}/\")\n",
    "else:\n",
    "    print(\"Error: task_index.csv missing. Run the Extractor first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e5485-d00e-4868-9e34-c0955553c3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
