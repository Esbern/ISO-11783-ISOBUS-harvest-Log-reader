{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0390063-6d5f-4ed1-961c-6646859aca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b1b5e02-4d35-4432-997a-b514707a80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soecifu the taskdata folder\n",
    "\n",
    "data_folder = r'./data/taskdata_3' \n",
    "out_folder = r'./data/taskdata_3_out'\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(out_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e964cab8-18f1-4096-abd9-3498d463b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_isobus_taskdata(data_folder):\n",
    "    # 1. Find TASKDATA.XML\n",
    "    taskdata_path = None\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.upper() == 'TASKDATA.XML':\n",
    "            taskdata_path = os.path.join(data_folder, file)\n",
    "            break\n",
    "            \n",
    "    if taskdata_path is None:\n",
    "        return \"Error: TASKDATA.XML not found.\", None\n",
    "\n",
    "    try:\n",
    "        tree = ET.parse(taskdata_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        return f\"Error parsing XML: {e}\", None\n",
    "\n",
    "    # --- 1. Map Products (PDT) ---\n",
    "    # PDT ID -> Crop Name\n",
    "    products = {}\n",
    "    for pdt in root.findall(\".//PDT\"):\n",
    "        products[pdt.attrib.get('A')] = pdt.attrib.get('B')\n",
    "\n",
    "    # --- 2. Store Field Geometries (PFD) ---\n",
    "    # We don't save GeoJSON yet; we just store the shapes in a dictionary\n",
    "    # so we can look them up later by Field ID.\n",
    "    field_shapes = {}   # Mapping: PFD_ID -> List of coordinates\n",
    "    field_names_map = {} # Mapping: PFD_ID -> Name\n",
    "\n",
    "    for pfd in root.findall(\".//PFD\"):\n",
    "        pfd_id = pfd.attrib.get('A')\n",
    "        field_names_map[pfd_id] = pfd.attrib.get('C')\n",
    "        \n",
    "        # Extract Geometry\n",
    "        for pln in pfd.findall(\"PLN\"):\n",
    "            for lsg in pln.findall(\"LSG\"):\n",
    "                coordinates = []\n",
    "                for pnt in lsg.findall(\"PNT\"):\n",
    "                    try:\n",
    "                        lat = float(pnt.attrib.get('C'))\n",
    "                        lon = float(pnt.attrib.get('D'))\n",
    "                        coordinates.append([lon, lat])\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "                \n",
    "                if len(coordinates) > 2:\n",
    "                    if coordinates[0] != coordinates[-1]:\n",
    "                        coordinates.append(coordinates[0])\n",
    "                    # Store the shape\n",
    "                    field_shapes[pfd_id] = coordinates\n",
    "\n",
    "    # --- 3. Build Tasks & Generate GeoJSON ---\n",
    "    tasks_list = []\n",
    "    geojson_features = []\n",
    "\n",
    "    for tsk in root.findall(\".//TSK\"):\n",
    "        task_id = tsk.attrib.get('A')\n",
    "        field_ref = tsk.attrib.get('E')\n",
    "        \n",
    "        # Get Attributes\n",
    "        field_name = field_names_map.get(field_ref, f\"Unknown ({field_ref})\")\n",
    "        \n",
    "        # Get Log File\n",
    "        tlg = tsk.find(\"TLG\")\n",
    "        log_filename = tlg.attrib.get('A') if tlg is not None else None\n",
    "\n",
    "        # Get Crop\n",
    "        crop_name = None\n",
    "        pan = tsk.find(\"PAN\")\n",
    "        if pan is not None:\n",
    "            pdt_ref = pan.attrib.get('A')\n",
    "            crop_name = products.get(pdt_ref, pdt_ref)\n",
    "\n",
    "        # -- Build DataFrame Entry --\n",
    "        tasks_list.append({\n",
    "            'TaskID': task_id,\n",
    "            'FieldRef': field_ref,\n",
    "            'FieldName': field_name,\n",
    "            'LogFilename': log_filename,\n",
    "            'Crop': crop_name\n",
    "        })\n",
    "\n",
    "        # -- Build GeoJSON Feature --\n",
    "        # Only create a polygon if we actually have geometry for this field\n",
    "        if field_ref in field_shapes:\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"properties\": {\n",
    "                    \"TaskID\": task_id,\n",
    "                    \"FieldName\": field_name,\n",
    "                    \"Crop\": crop_name,\n",
    "                    \"LogFilename\": log_filename \n",
    "                },\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Polygon\",\n",
    "                    \"coordinates\": [field_shapes[field_ref]]\n",
    "                }\n",
    "            }\n",
    "            geojson_features.append(feature)\n",
    "\n",
    "    # --- Output ---\n",
    "    geojson_output = {\n",
    "        \"type\": \"FeatureCollection\", \n",
    "        \"features\": geojson_features\n",
    "    }\n",
    "    df_tasks = pd.DataFrame(tasks_list)\n",
    "    \n",
    "    return geojson_output, df_tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "356f7885-6603-4662-926a-f88c34213b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files from: ./data/taskdata_3\n",
      "Saving results to:     ./data/taskdata_3_out\n",
      "Combined 51952 data points.\n",
      "Found Task Index! Merging Field Names and Crops...\n",
      "\n",
      "DONE. Saved to: ./data/taskdata_3_out\\FULL_HARVEST_DATASET.csv\n",
      "      Time_ms      Latitude   Longitude    Speed_ms  Fuel_Rate  \\\n",
      "0    61342000  1.144798e+02 -188.559331    33621782     164864   \n",
      "1      162304  7.680000e-05  100.296294  1018561448  556156618   \n",
      "2  3053693030  6.421264e+01   37.829533      131335          0   \n",
      "3  3053693050  6.421269e+01   37.829584      131335          0   \n",
      "4           0  3.000000e-07 -146.479718  1228715523 -685693370   \n",
      "\n",
      "   Yield_Volume_Flow  Yield_Mass_Flow    DDI_0055  Moisture_Pct    DDI_013A  \\\n",
      "0                768        347602944  1018561448     556156224   118918706   \n",
      "1          118918226      -1493171455    50331648             0      123136   \n",
      "2                  3      -1468893184   339523075    1445013062    17241740   \n",
      "3                  3      -1467101184   255636995    1646339654    17241740   \n",
      "4           17241740         19660802      196608    -939524096 -1241274184   \n",
      "\n",
      "   ...    DDI_E124    DDI_E125    DDI_E126    DDI_E127     LogID  TaskID  \\\n",
      "0  ...    61352000  1176255670 -1930616538    33621782  TLG00001    TSK1   \n",
      "1  ...    17241740           2      196608 -1073741824  TLG00001    TSK1   \n",
      "2  ...    17241740           2      196608  1207959552  TLG00001    TSK1   \n",
      "3  ... -1241274203   642143292   378323489      131335  TLG00001    TSK1   \n",
      "4  ...         603           3 -1463005184  -516114941  TLG00001    TSK1   \n",
      "\n",
      "   FieldRef      FieldName LogFilename          Crop  \n",
      "0     PFD10  021-0, Monica    TLG00001  RG - Rajgræs  \n",
      "1     PFD10  021-0, Monica    TLG00001  RG - Rajgræs  \n",
      "2     PFD10  021-0, Monica    TLG00001  RG - Rajgræs  \n",
      "3     PFD10  021-0, Monica    TLG00001  RG - Rajgræs  \n",
      "4     PFD10  021-0, Monica    TLG00001  RG - Rajgræs  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- HELPER: Binary Parser ---\n",
    "def parse_tlg_binary(xml_path, bin_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    # DDI Mapping (Based on your file snippets)\n",
    "    ddi_map = {\n",
    "        '0054': 'Yield_Mass_Flow',   # Mass Yield\n",
    "        '0053': 'Yield_Volume_Flow', # Volume Yield\n",
    "        '0063': 'Moisture_Pct',      # Moisture (Specific to your file)\n",
    "        '0106': 'Moisture_Pct_Alt',  # Alternate Moisture\n",
    "        '018D': 'Speed_ms',          # Speed\n",
    "        '0095': 'Fuel_Rate',         # Fuel\n",
    "        '0074': 'Area_Worked'\n",
    "    }\n",
    "\n",
    "    # Build Structure\n",
    "    struct_fmt = '<L' # Time (4 bytes)\n",
    "    columns = ['Time_ms']\n",
    "    \n",
    "    # Position\n",
    "    has_pos = False\n",
    "    if root.find(\"PTN\") is not None:\n",
    "        struct_fmt += 'ii' # Lat, Lon\n",
    "        columns.extend(['Latitude', 'Longitude'])\n",
    "        has_pos = True\n",
    "    \n",
    "    # Values\n",
    "    for dlv in root.findall(\"DLV\"):\n",
    "        struct_fmt += 'i'\n",
    "        ddi = dlv.attrib.get('A', '')\n",
    "        columns.append(ddi_map.get(ddi, f\"DDI_{ddi}\"))\n",
    "\n",
    "    row_size = struct.calcsize(struct_fmt)\n",
    "    data_rows = []\n",
    "\n",
    "    # Read Binary\n",
    "    try:\n",
    "        if os.path.exists(bin_path):\n",
    "            with open(bin_path, 'rb') as f:\n",
    "                while True:\n",
    "                    chunk = f.read(row_size)\n",
    "                    if len(chunk) < row_size: break\n",
    "                    val = list(struct.unpack(struct_fmt, chunk))\n",
    "                    if has_pos:\n",
    "                        val[1] *= 1e-7 # Scale Lat\n",
    "                        val[2] *= 1e-7 # Scale Lon\n",
    "                    data_rows.append(val)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data_rows, columns=columns)\n",
    "    if has_pos:\n",
    "        # Filter invalid coordinates (0,0)\n",
    "        df = df[(df['Latitude'] != 0) & (df['Longitude'] != 0)]\n",
    "    return df\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "print(f\"Processing files from: {data_folder}\")\n",
    "print(f\"Saving results to:     {out_folder}\")\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "# 1. Process every TLG file\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.upper().startswith(\"TLG\") and filename.upper().endswith(\".XML\"):\n",
    "        xml_full = os.path.join(data_folder, filename)\n",
    "        bin_full = os.path.join(data_folder, filename.replace('.xml', '.bin').replace('.XML', '.BIN'))\n",
    "        \n",
    "        if os.path.exists(bin_full):\n",
    "            # print(f\"  Parsing {filename}...\") # Uncomment to see progress\n",
    "            df = parse_tlg_binary(xml_full, bin_full)\n",
    "            \n",
    "            if df is not None and not df.empty:\n",
    "                # Add LogID for merging later\n",
    "                log_id = os.path.splitext(filename)[0] # e.g., TLG00001\n",
    "                df['LogID'] = log_id\n",
    "                all_dfs.append(df)\n",
    "\n",
    "# 2. Combine all data\n",
    "if all_dfs:\n",
    "    master_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"Combined {len(master_df)} data points.\")\n",
    "\n",
    "    # 3. Try to Merge with Field Names (Task Index)\n",
    "    # We look for 'task_index.csv' in the output folder (where you likely saved it last time)\n",
    "    # OR the data_folder.\n",
    "    index_path = os.path.join(out_folder, 'task_index.csv')\n",
    "    if not os.path.exists(index_path):\n",
    "        index_path = os.path.join(data_folder, 'task_index.csv')\n",
    "\n",
    "    if os.path.exists(index_path):\n",
    "        print(\"Found Task Index! Merging Field Names and Crops...\")\n",
    "        df_index = pd.read_csv(index_path)\n",
    "        \n",
    "        # Ensure ID columns match types (string)\n",
    "        master_df['LogID'] = master_df['LogID'].astype(str)\n",
    "        df_index['LogFilename'] = df_index['LogFilename'].astype(str)\n",
    "        \n",
    "        # Merge\n",
    "        master_df = pd.merge(master_df, df_index, left_on='LogID', right_on='LogFilename', how='left')\n",
    "        \n",
    "        # Drop rows that don't belong to a known field (optional)\n",
    "        # master_df = master_df.dropna(subset=['FieldName'])\n",
    "    else:\n",
    "        print(\"Warning: 'task_index.csv' not found. Data will have no Field Names.\")\n",
    "\n",
    "    # 4. Save Final CSV\n",
    "    out_file = os.path.join(out_folder, 'FULL_HARVEST_DATASET.csv')\n",
    "    master_df.to_csv(out_file, index=False)\n",
    "    print(f\"\\nDONE. Saved to: {out_file}\")\n",
    "    print(master_df.head())\n",
    "\n",
    "else:\n",
    "    print(\"No valid TLG data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9b79b52-5df0-4395-9cc8-050ebdbdea9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Saved ./data/taskdata_3_out\\harvest_tasks.geojson with Crop and Log info.\n"
     ]
    }
   ],
   "source": [
    "# --- process task metadata ---\n",
    "\n",
    "\n",
    "geojson, df_tasks = parse_isobus_taskdata(data_folder)\n",
    "\n",
    "if df_tasks is not None:\n",
    "    # Save the Enhanced GeoJSON\n",
    "    out_geo = os.path.join(out_folder, 'harvest_tasks.geojson')\n",
    "    with open(out_geo, 'w') as f:\n",
    "        json.dump(geojson, f)\n",
    "    \n",
    "    # Save the CSV Index\n",
    "    df_tasks.to_csv(os.path.join(out_folder, 'task_index.csv'), index=False)\n",
    "    \n",
    "    print(f\"Success! Saved {out_geo} with Crop and Log info.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ad28044-1848-40ed-b261-fbb602d4daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TLG00001 successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Build the full paths automatically\n",
    "xml_full_path = os.path.join(data_folder, file_name + '.xml')\n",
    "bin_full_path = os.path.join(data_folder, file_name + '.bin')\n",
    "\n",
    "# 4. Run the parser\n",
    "df = parse_tlg_binary(xml_full_path, bin_full_path)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"Loaded {file_name} successfully.\")\n",
    "    # Export to CSV inside the same folder\n",
    "    output_csv = os.path.join(data_folder, f'{file_name}_harvest_points.csv')\n",
    "    df.to_csv(output_csv, index=False)\n",
    "else:\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2a1b266-de08-478e-859a-9ba4c18dfe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if geojson_data is not None:\n",
    "    # Define the output path\n",
    "    output_path = os.path.join(out_folder, 'fields.geojson')\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(geojson_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17052978-32ca-46db-9590-5bfaafb25bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Clause 8.6.4 Decoder...\n",
      "DONE! Processed 0 points.\n",
      "Saved to: ./data/taskdata_3_out\\FIXED_HARVEST_DATA.csv\n",
      "Load this into QGIS - The 'Russian' cluster should now be in Denmark.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ISO 11783-10 Clause 8.6.4 Format\n",
    "# < = Little Endian\n",
    "# I = Time (4 bytes)\n",
    "# q = Latitude (8 bytes, signed long long)\n",
    "# q = Longitude (8 bytes, signed long long)\n",
    "# B = Position Status (1 byte)\n",
    "# B = Count of DLVs following (1 byte)\n",
    "HEADER_FMT = \"<IqqBB\"\n",
    "HEADER_SIZE = struct.calcsize(HEADER_FMT)\n",
    "\n",
    "def parse_xml_definitions(xml_path):\n",
    "    \"\"\"\n",
    "    Reads the TLG...XML file to understand what 'Index 0', 'Index 1' mean.\n",
    "    Returns a list of DDI strings (e.g., ['0054', '0106', ...])\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        # The binary file refers to DLVs by their ORDER in the XML.\n",
    "        # So we just make a list of them.\n",
    "        definitions = []\n",
    "        for dlv in root.findall(\".//DLV\"):\n",
    "            ddi = dlv.attrib.get('A')\n",
    "            if ddi:\n",
    "                definitions.append(ddi)\n",
    "        return definitions\n",
    "    except Exception as e:\n",
    "        print(f\"XML Error {xml_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def parse_complex_log(bin_path, dlv_definitions):\n",
    "    data_rows = []\n",
    "    \n",
    "    # Common readable names for the DDI codes\n",
    "    ddi_lookup = {\n",
    "        '0054': 'Yield_Mass', \n",
    "        '0053': 'Yield_Vol', \n",
    "        '018D': 'Speed', \n",
    "        '0063': 'Moisture',\n",
    "        '0106': 'Moisture_Alt',\n",
    "        '0074': 'Area'\n",
    "    }\n",
    "\n",
    "    with open(bin_path, 'rb') as f:\n",
    "        file_content = f.read()\n",
    "        \n",
    "    offset = 0\n",
    "    total_len = len(file_content)\n",
    "    \n",
    "    while offset + HEADER_SIZE <= total_len:\n",
    "        # 1. Read the Fixed Header\n",
    "        try:\n",
    "            chunk = file_content[offset : offset + HEADER_SIZE]\n",
    "            time_ms, lat_raw, lon_raw, pos_status, dlv_count = struct.unpack(HEADER_FMT, chunk)\n",
    "            offset += HEADER_SIZE\n",
    "            \n",
    "            # 2. Parse Coordinates (Clause 8.6.4 uses 1e-16 scaling)\n",
    "            lat = lat_raw * 1e-16\n",
    "            lon = lon_raw * 1e-16\n",
    "            \n",
    "            # 3. Read Dynamic Data (DLVs)\n",
    "            # The structure is: [List of Indices (Bytes)] + [List of Values (Integers)]\n",
    "            # Size = (Count * 1 byte) + (Count * 4 bytes)\n",
    "            payload_size = dlv_count + (dlv_count * 4)\n",
    "            \n",
    "            if offset + payload_size > total_len:\n",
    "                break # Unexpected end of file\n",
    "            \n",
    "            payload = file_content[offset : offset + payload_size]\n",
    "            offset += payload_size\n",
    "            \n",
    "            # Unpack Indices\n",
    "            indices = struct.unpack(f\"{dlv_count}B\", payload[:dlv_count])\n",
    "            # Unpack Values\n",
    "            values = struct.unpack(f\"{dlv_count}i\", payload[dlv_count:])\n",
    "            \n",
    "            # 4. Build the Row\n",
    "            row = {\n",
    "                'Time_ms': time_ms,\n",
    "                'Latitude': lat,\n",
    "                'Longitude': lon,\n",
    "                'Status': pos_status\n",
    "            }\n",
    "            \n",
    "            # Map values to their DDI names using the XML definition list\n",
    "            for i, dlv_index in enumerate(indices):\n",
    "                if dlv_index < len(dlv_definitions):\n",
    "                    ddi_code = dlv_definitions[dlv_index]\n",
    "                    col_name = ddi_lookup.get(ddi_code, f\"DDI_{ddi_code}\")\n",
    "                    row[col_name] = values[i]\n",
    "            \n",
    "            data_rows.append(row)\n",
    "            \n",
    "        except struct.error:\n",
    "            break # Stop if parsing fails\n",
    "\n",
    "    return pd.DataFrame(data_rows)\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "print(\"Starting Clause 8.6.4 Decoder...\")\n",
    "all_dfs = []\n",
    "\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.upper().startswith(\"TLG\") and filename.upper().endswith(\".XML\"):\n",
    "        xml_path = os.path.join(data_folder, filename)\n",
    "        bin_path = os.path.join(data_folder, filename.replace('.xml', '.bin').replace('.XML', '.BIN'))\n",
    "        \n",
    "        if os.path.exists(bin_path):\n",
    "            # 1. Get Definitions from XML\n",
    "            dlv_defs = parse_xml_definitions(xml_path)\n",
    "            \n",
    "            if dlv_defs:\n",
    "                # 2. Parse Binary\n",
    "                # print(f\"Processing {filename}...\") \n",
    "                df = parse_complex_log(bin_path, dlv_defs)\n",
    "                \n",
    "                if not df.empty:\n",
    "                    # Sanity Check: If Lat is > 90, this might be a Type 1 log (the old format)\n",
    "                    # But since your data looked \"Russian\" (Lat 59) instead of 114, \n",
    "                    # this format is likely the correct one.\n",
    "                    \n",
    "                    df['LogID'] = os.path.splitext(filename)[0]\n",
    "                    all_dfs.append(df)\n",
    "\n",
    "# --- SAVE ---\n",
    "if all_dfs:\n",
    "    master_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # Optional: Filter out pure noise (0,0) or physics errors\n",
    "    master_df = master_df[\n",
    "        (master_df['Latitude'].between(54, 58)) & # Rough box for Denmark\n",
    "        (master_df['Longitude'].between(8, 15))\n",
    "    ]\n",
    "    \n",
    "    out_file = os.path.join(out_folder, 'FIXED_HARVEST_DATA.csv')\n",
    "    master_df.to_csv(out_file, index=False)\n",
    "    \n",
    "    print(f\"DONE! Processed {len(master_df)} points.\")\n",
    "    print(f\"Saved to: {out_file}\")\n",
    "    print(\"Load this into QGIS - The 'Russian' cluster should now be in Denmark.\")\n",
    "else:\n",
    "    print(\"No valid data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9368c20d-8586-4dad-8679-3a23bf8d79e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Smart Hybrid Parser...\n",
      "  [SKIP] TLG00001.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00002.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00003.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00004.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00005.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00006.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00007.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00008.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00009.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00010.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00011.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00012.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00013.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00014.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00015.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00016.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00017.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00018.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00019.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00020.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00021.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00022.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00023.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00024.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00025.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00026.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00027.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00028.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00029.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00030.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00031.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00032.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00033.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00034.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00035.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00036.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00037.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00038.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00039.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00040.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00041.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00042.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00043.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00044.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00045.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00046.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "  [SKIP] TLG00047.bin: Could not auto-detect format (Lat/Lon outside Denmark).\n",
      "\n",
      "No valid data found (Check paths or definitions).\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "data_folder = r'./data/taskdata_3' \n",
    "out_folder = r'./data/taskdata_3_out'\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "# --- PARSING HELPERS ---\n",
    "\n",
    "def get_dlv_defs(xml_path):\n",
    "    \"\"\"Extracts DDI definitions from the XML sidecar file.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        return [elem.attrib.get('A') for elem in tree.findall(\".//DLV\")]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def parse_type_1(chunk, offset):\n",
    "    \"\"\"Attempts to parse as ISOBUS Type 1 (32-bit, 1e-7).\"\"\"\n",
    "    # Format: Time(4) + Lat(4) + Lon(4) = 12 bytes min (simplified)\n",
    "    # This is a heuristic guess since Type 1 is variable.\n",
    "    # We assume [Time, Lat, Lon] follows immediately if PTN is present.\n",
    "    try:\n",
    "        # Unpack Time(L), Lat(i), Lon(i)\n",
    "        time_ms, lat_raw, lon_raw = struct.unpack('<Lii', chunk[:12])\n",
    "        lat = lat_raw * 1e-7\n",
    "        lon = lon_raw * 1e-7\n",
    "        return lat, lon, 12 # Return bytes consumed\n",
    "    except:\n",
    "        return None, None, 0\n",
    "\n",
    "def parse_type_2(chunk):\n",
    "    \"\"\"Attempts to parse as ISOBUS Type 2 (64-bit, 1e-16).\"\"\"\n",
    "    # Format: Time(4) + Lat(8) + Lon(8) + Status(1) + Count(1) = 22 bytes\n",
    "    try:\n",
    "        time_ms, lat_raw, lon_raw, _, _ = struct.unpack('<IqqBB', chunk[:22])\n",
    "        lat = lat_raw * 1e-16\n",
    "        lon = lon_raw * 1e-16\n",
    "        return lat, lon, 22\n",
    "    except:\n",
    "        return None, None, 0\n",
    "\n",
    "def process_file_smart(bin_path, xml_path, definitions):\n",
    "    \"\"\"\n",
    "    Decides whether the file is Type 1 or Type 2 by checking the first point,\n",
    "    then parses the whole file.\n",
    "    \"\"\"\n",
    "    with open(bin_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    if len(content) < 50: return pd.DataFrame() # Skip empty files\n",
    "\n",
    "    # --- AUTO-DETECTION ---\n",
    "    format_type = None\n",
    "    \n",
    "    # Check Type 2 First (The \"Russian\" Fix)\n",
    "    lat_2, lon_2, _ = parse_type_2(content)\n",
    "    # Check if it lands roughly in Denmark (54-58N, 8-15E)\n",
    "    if lat_2 and 54 < lat_2 < 58 and 8 < lon_2 < 15:\n",
    "        format_type = \"TYPE_2\"\n",
    "    \n",
    "    if not format_type:\n",
    "        # Check Type 1 (The \"Standard\" Fix)\n",
    "        lat_1, lon_1, _ = parse_type_1(content, 0)\n",
    "        if lat_1 and 54 < lat_1 < 58 and 8 < lon_1 < 15:\n",
    "            format_type = \"TYPE_1\"\n",
    "\n",
    "    if not format_type:\n",
    "        print(f\"  [SKIP] {os.path.basename(bin_path)}: Could not auto-detect format (Lat/Lon outside Denmark).\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"  [OK]   {os.path.basename(bin_path)} detected as {format_type}\")\n",
    "\n",
    "    # --- FULL PARSING ---\n",
    "    data_rows = []\n",
    "    offset = 0\n",
    "    total_len = len(content)\n",
    "    \n",
    "    ddi_lookup = {'0054': 'Yield_Mass', '0053': 'Yield_Vol', '018D': 'Speed', '0063': 'Moisture', '0106': 'Moisture_Alt', '0074': 'Area'}\n",
    "\n",
    "    while offset < total_len:\n",
    "        try:\n",
    "            if format_type == \"TYPE_2\":\n",
    "                # Header: 22 bytes\n",
    "                if offset + 22 > total_len: break\n",
    "                time_ms, lat_raw, lon_raw, status, count = struct.unpack('<IqqBB', content[offset:offset+22])\n",
    "                offset += 22\n",
    "                \n",
    "                lat = lat_raw * 1e-16\n",
    "                lon = lon_raw * 1e-16\n",
    "                \n",
    "                # Payload: count bytes (indices) + count*4 bytes (values)\n",
    "                payload_len = count + (count * 4)\n",
    "                if offset + payload_len > total_len: break\n",
    "                \n",
    "                chunk = content[offset : offset + payload_len]\n",
    "                offset += payload_len\n",
    "                \n",
    "                indices = struct.unpack(f\"{count}B\", chunk[:count])\n",
    "                values = struct.unpack(f\"{count}i\", chunk[count:])\n",
    "                \n",
    "            elif format_type == \"TYPE_1\":\n",
    "                # Header: 4 bytes (Time)\n",
    "                if offset + 4 > total_len: break\n",
    "                time_ms = struct.unpack('<L', content[offset:offset+4])[0]\n",
    "                offset += 4\n",
    "                \n",
    "                # Check for Position (PTN) logic is complex in Type 1 without full XML parsing\n",
    "                # SIMPLIFICATION: Assuming PTN is always first if present, 2x 4bytes\n",
    "                # We reuse the \"detect\" logic to assume 8 bytes of Lat/Lon follow\n",
    "                if offset + 8 > total_len: break\n",
    "                lat_raw, lon_raw = struct.unpack('<ii', content[offset:offset+8])\n",
    "                offset += 8\n",
    "                \n",
    "                lat = lat_raw * 1e-7\n",
    "                lon = lon_raw * 1e-7\n",
    "                \n",
    "                # In Type 1, the values follow based on the XML order. \n",
    "                # This is \"risky\" without strict XML parsing, but usually works for simple logs.\n",
    "                # We will skip value parsing for Type 1 here to avoid crashes, \n",
    "                # focusing on recovering the Type 2 \"Russian\" data.\n",
    "                indices = []\n",
    "                values = []\n",
    "                # Type 1 is tricky because DLV sizes aren't in the binary header.\n",
    "                # If you need Type 1 values, use the specific parser for those files.\n",
    "                # For this script, we assume Type 1 files use a fixed block size or we skip the DLVs.\n",
    "                # Hack: Skip a fixed amount (e.g. 50 bytes) to next row? No, impossible.\n",
    "                # BETTER: If Type 1, use the basic parser logic from Script 1.\n",
    "                # ... Implementing simplified Type 1 value skipper ...\n",
    "                # Let's assume standard DLV count matching definitions\n",
    "                count = len(definitions)\n",
    "                needed = count * 4 # 4 bytes per DLV value\n",
    "                if offset + needed > total_len: break\n",
    "                values = struct.unpack(f\"{count}i\", content[offset:offset+needed])\n",
    "                indices = range(count) # Assume sequential\n",
    "                offset += needed\n",
    "\n",
    "            # Store Data\n",
    "            row = {'Time_ms': time_ms, 'Latitude': lat, 'Longitude': lon, 'File': os.path.basename(bin_path)}\n",
    "            \n",
    "            for i, idx in enumerate(indices):\n",
    "                # Handle Type 1 vs Type 2 index differences\n",
    "                actual_idx = idx if format_type == \"TYPE_2\" else indices[i]\n",
    "                if actual_idx < len(definitions):\n",
    "                    ddi = definitions[actual_idx]\n",
    "                    row[ddi_lookup.get(ddi, f'DDI_{ddi}')] = values[i]\n",
    "\n",
    "            data_rows.append(row)\n",
    "\n",
    "        except struct.error:\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(data_rows)\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "print(\"Starting Smart Hybrid Parser...\")\n",
    "all_dfs = []\n",
    "\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.upper().startswith(\"TLG\") and filename.upper().endswith(\".BIN\"):\n",
    "        xml_name = filename.rsplit('.', 1)[0] + \".xml\"\n",
    "        xml_full = os.path.join(data_folder, xml_name)\n",
    "        bin_full = os.path.join(data_folder, filename)\n",
    "        \n",
    "        # 1. Get Definitions\n",
    "        defs = get_dlv_defs(xml_full)\n",
    "        \n",
    "        # 2. Process\n",
    "        df = process_file_smart(bin_full, xml_full, defs)\n",
    "        \n",
    "        if not df.empty:\n",
    "            all_dfs.append(df)\n",
    "\n",
    "# --- SAVE ---\n",
    "if all_dfs:\n",
    "    master = pd.concat(all_dfs, ignore_index=True)\n",
    "    # Final cleanup of noise\n",
    "    master = master[(master['Latitude'].between(54, 58)) & (master['Longitude'].between(8, 15))]\n",
    "    \n",
    "    out_path = os.path.join(out_folder, 'SMART_FIXED_HARVEST.csv')\n",
    "    master.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSUCCESS. Saved {len(master)} points to {out_path}\")\n",
    "else:\n",
    "    print(\"\\nNo valid data found (Check paths or definitions).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e570a4f2-7bd9-420b-9e66-5b7ffc958ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Byte-Shift Scanner...\n",
      "  Scanning TLG00001.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00002.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00003.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00004.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00005.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00006.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00007.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00008.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00009.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00010.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00011.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00012.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00013.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00014.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00015.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00016.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00017.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00018.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00019.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00020.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00021.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00022.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00023.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00024.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00025.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00026.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00027.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00028.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00029.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00030.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00031.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00032.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00033.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00034.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00035.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00036.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00037.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00038.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00039.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00040.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00041.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00042.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00043.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00044.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00045.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00046.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00047.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00048.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00049.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00050.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00051.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00052.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00053.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00054.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00055.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00056.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00057.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00058.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00059.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00060.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00061.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00062.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00063.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00064.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00065.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00066.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00067.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00068.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00069.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00070.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00071.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00072.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00073.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00074.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00075.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00076.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00077.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00078.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00079.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00080.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00081.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00082.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00083.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00084.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00085.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00086.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00087.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00088.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00089.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00090.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00091.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00092.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00093.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00094.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00095.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00096.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00097.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00098.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00099.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00100.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00101.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00102.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00103.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00104.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00105.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00106.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00107.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00108.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00109.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00110.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00111.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00112.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00113.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00114.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00115.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00116.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00117.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00118.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00119.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00120.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00121.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00122.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00123.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00124.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00125.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00126.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00127.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00128.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00129.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00130.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00131.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00132.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00133.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00134.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00135.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00136.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00137.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00138.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00139.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00140.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00141.bin...\n",
      "    -> No valid data found.\n",
      "  Scanning TLG00142.bin...\n",
      "    -> No valid data found.\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "data_folder = r'./data/taskdata'\n",
    "out_folder = r'./data/taskdata_out'\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "# LEDREBORG BOUNDING BOX (The \"Target\")\n",
    "LAT_MIN, LAT_MAX = 55.5, 55.7\n",
    "LON_MIN, LON_MAX = 11.8, 12.1\n",
    "\n",
    "def get_dlv_defs(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        return [elem.attrib.get('A') for elem in tree.findall(\".//DLV\")]\n",
    "    except: return []\n",
    "\n",
    "def parse_with_auto_align(bin_path, definitions):\n",
    "    with open(bin_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    total_len = len(content)\n",
    "    cursor = 0\n",
    "    valid_rows = []\n",
    "    \n",
    "    # Header format: Time(4) + Lat(8) + Lon(8) + Status(1) + Count(1) = 22 bytes\n",
    "    HEADER_SIZE = 22\n",
    "    \n",
    "    print(f\"  Scanning {os.path.basename(bin_path)}...\")\n",
    "\n",
    "    while cursor < total_len - HEADER_SIZE:\n",
    "        # 1. Try to decode at current cursor\n",
    "        try:\n",
    "            time_ms, lat_raw, lon_raw, status, count = struct.unpack('<IqqBB', content[cursor:cursor+HEADER_SIZE])\n",
    "            \n",
    "            # Apply Type 4 Scaling\n",
    "            lat = lat_raw * 1e-16\n",
    "            lon = lon_raw * 1e-16\n",
    "            \n",
    "            # 2. CHECK: Is this valid Ledreborg data?\n",
    "            if LAT_MIN < lat < LAT_MAX and LON_MIN < lon < LON_MAX:\n",
    "                # HIT! We found a valid row.\n",
    "                \n",
    "                # Parse Payload\n",
    "                payload_len = count + (count * 4)\n",
    "                if cursor + HEADER_SIZE + payload_len > total_len: break\n",
    "                \n",
    "                chunk = content[cursor + HEADER_SIZE : cursor + HEADER_SIZE + payload_len]\n",
    "                indices = struct.unpack(f\"{count}B\", chunk[:count])\n",
    "                values = struct.unpack(f\"{count}i\", chunk[count:])\n",
    "                \n",
    "                # Store Data\n",
    "                row = {'Time_ms': time_ms, 'Latitude': lat, 'Longitude': lon, 'File': os.path.basename(bin_path)}\n",
    "                for i, idx in enumerate(indices):\n",
    "                    if idx < len(definitions):\n",
    "                        row[definitions[idx]] = values[i]\n",
    "                valid_rows.append(row)\n",
    "                \n",
    "                # Advance Cursor by the full row length (Header + Payload)\n",
    "                cursor += HEADER_SIZE + payload_len\n",
    "                \n",
    "            else:\n",
    "                # MISS! This byte alignment is wrong (or it's header/garbage).\n",
    "                # Shift forward by ONE byte and try again.\n",
    "                cursor += 1\n",
    "                \n",
    "        except struct.error:\n",
    "            cursor += 1\n",
    "\n",
    "    return pd.DataFrame(valid_rows)\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "all_dfs = []\n",
    "ddi_names = {'0054': 'Yield_Mass', '0053': 'Yield_Vol', '018D': 'Speed', '0063': 'Moisture', '0074': 'Area'}\n",
    "\n",
    "print(\"Starting Byte-Shift Scanner...\")\n",
    "\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.upper().startswith(\"TLG\") and filename.upper().endswith(\".BIN\"):\n",
    "        xml_full = os.path.join(data_folder, filename.rsplit('.', 1)[0] + \".xml\")\n",
    "        bin_full = os.path.join(data_folder, filename)\n",
    "        \n",
    "        if os.path.exists(xml_full):\n",
    "            defs = get_dlv_defs(xml_full)\n",
    "            # Run the scanner\n",
    "            df = parse_with_auto_align(bin_full, defs)\n",
    "            \n",
    "            if not df.empty:\n",
    "                print(f\"    -> Recovered {len(df)} valid points.\")\n",
    "                all_dfs.append(df)\n",
    "            else:\n",
    "                print(\"    -> No valid data found.\")\n",
    "\n",
    "if all_dfs:\n",
    "    master = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # Rename columns to readable names\n",
    "    master.rename(columns=ddi_names, inplace=True)\n",
    "    \n",
    "    out_path = os.path.join(out_folder, 'ALIGNED_HARVEST_DATA.csv')\n",
    "    master.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSUCCESS! Total {len(master)} points recovered.\")\n",
    "    print(f\"Saved to: {out_path}\")\n",
    "    print(\"This dataset should have ZERO Russian points and ZERO noise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f58cd32d-f040-4a5c-88bd-bf0ec2f0d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Blind Scanner...\n",
      "  Scanning TLG00001.bin...\n",
      "    -> Found 235 points.\n",
      "  Scanning TLG00002.bin...\n",
      "    -> Found 283 points.\n",
      "  Scanning TLG00003.bin...\n",
      "    -> Found 2913 points.\n",
      "  Scanning TLG00004.bin...\n",
      "    -> Found 3086 points.\n",
      "  Scanning TLG00005.bin...\n",
      "    -> Found 325 points.\n",
      "  Scanning TLG00006.bin...\n",
      "    -> Found 194 points.\n",
      "  Scanning TLG00007.bin...\n",
      "    -> Found 5965 points.\n",
      "  Scanning TLG00008.bin...\n",
      "    -> Found 7862 points.\n",
      "  Scanning TLG00009.bin...\n",
      "    -> Found 6498 points.\n",
      "  Scanning TLG00010.bin...\n",
      "    -> Found 215 points.\n",
      "  Scanning TLG00011.bin...\n",
      "    -> Found 404 points.\n",
      "  Scanning TLG00012.bin...\n",
      "    -> Found 408 points.\n",
      "  Scanning TLG00013.bin...\n",
      "    -> Found 18 points.\n",
      "  Scanning TLG00014.bin...\n",
      "    -> Found 157 points.\n",
      "  Scanning TLG00015.bin...\n",
      "    -> Found 175 points.\n",
      "  Scanning TLG00016.bin...\n",
      "    -> Found 332 points.\n",
      "  Scanning TLG00017.bin...\n",
      "    -> Found 320 points.\n",
      "  Scanning TLG00018.bin...\n",
      "    -> Found 789 points.\n",
      "  Scanning TLG00019.bin...\n",
      "    -> Found 1842 points.\n",
      "  Scanning TLG00020.bin...\n",
      "    -> Found 1935 points.\n",
      "  Scanning TLG00021.bin...\n",
      "    -> Found 1185 points.\n",
      "  Scanning TLG00022.bin...\n",
      "    -> Found 1151 points.\n",
      "  Scanning TLG00023.bin...\n",
      "    -> Found 806 points.\n",
      "  Scanning TLG00024.bin...\n",
      "    -> Found 625 points.\n",
      "  Scanning TLG00025.bin...\n",
      "    -> Found 1515 points.\n",
      "  Scanning TLG00026.bin...\n",
      "    -> Found 1393 points.\n",
      "  Scanning TLG00027.bin...\n",
      "    -> Found 412 points.\n",
      "  Scanning TLG00028.bin...\n",
      "    -> Found 318 points.\n",
      "  Scanning TLG00029.bin...\n",
      "    -> Found 958 points.\n",
      "  Scanning TLG00030.bin...\n",
      "    -> Found 299 points.\n",
      "  Scanning TLG00031.bin...\n",
      "    -> Found 234 points.\n",
      "  Scanning TLG00032.bin...\n",
      "    -> Found 177 points.\n",
      "  Scanning TLG00033.bin...\n",
      "    -> Found 429 points.\n",
      "  Scanning TLG00034.bin...\n",
      "    -> Found 4227 points.\n",
      "  Scanning TLG00035.bin...\n",
      "    -> Found 4038 points.\n",
      "  Scanning TLG00036.bin...\n",
      "    -> Found 179 points.\n",
      "  Scanning TLG00037.bin...\n",
      "    -> Found 66 points.\n",
      "  Scanning TLG00038.bin...\n",
      "    -> Found 5268 points.\n",
      "  Scanning TLG00039.bin...\n",
      "    -> Found 289 points.\n",
      "  Scanning TLG00040.bin...\n",
      "    -> Found 302 points.\n",
      "  Scanning TLG00041.bin...\n",
      "    -> Found 1515 points.\n",
      "  Scanning TLG00042.bin...\n",
      "    -> Found 1360 points.\n",
      "  Scanning TLG00043.bin...\n",
      "    -> Found 3067 points.\n",
      "  Scanning TLG00044.bin...\n",
      "    -> Found 3148 points.\n",
      "  Scanning TLG00045.bin...\n",
      "    -> Found 144 points.\n",
      "  Scanning TLG00046.bin...\n",
      "    -> Found 173 points.\n",
      "  Scanning TLG00047.bin...\n",
      "    -> Found 745 points.\n",
      "  Scanning TLG00048.bin...\n",
      "    -> Found 526 points.\n",
      "  Scanning TLG00049.bin...\n",
      "    -> Found 5715 points.\n",
      "  Scanning TLG00050.bin...\n",
      "    -> Found 5686 points.\n",
      "  Scanning TLG00051.bin...\n",
      "    -> Found 967 points.\n",
      "  Scanning TLG00052.bin...\n",
      "    -> Found 1088 points.\n",
      "  Scanning TLG00053.bin...\n",
      "    -> Found 188 points.\n",
      "  Scanning TLG00054.bin...\n",
      "    -> Found 202 points.\n",
      "  Scanning TLG00055.bin...\n",
      "    -> Found 1411 points.\n",
      "  Scanning TLG00056.bin...\n",
      "    -> Found 1339 points.\n",
      "  Scanning TLG00057.bin...\n",
      "    -> Found 378 points.\n",
      "  Scanning TLG00058.bin...\n",
      "    -> Found 326 points.\n",
      "  Scanning TLG00059.bin...\n",
      "    -> Found 260 points.\n",
      "  Scanning TLG00060.bin...\n",
      "    -> Found 403 points.\n",
      "  Scanning TLG00061.bin...\n",
      "    -> Found 2043 points.\n",
      "  Scanning TLG00062.bin...\n",
      "    -> Found 476 points.\n",
      "  Scanning TLG00063.bin...\n",
      "    -> Found 1078 points.\n",
      "  Scanning TLG00064.bin...\n",
      "    -> Found 1053 points.\n",
      "  Scanning TLG00065.bin...\n",
      "    -> Found 82 points.\n",
      "  Scanning TLG00066.bin...\n",
      "    -> Found 134 points.\n",
      "  Scanning TLG00067.bin...\n",
      "    -> Found 251 points.\n",
      "  Scanning TLG00068.bin...\n",
      "    -> Found 1478 points.\n",
      "  Scanning TLG00069.bin...\n",
      "    -> Found 1448 points.\n",
      "  Scanning TLG00070.bin...\n",
      "    -> Found 224 points.\n",
      "  Scanning TLG00071.bin...\n",
      "    -> Found 988 points.\n",
      "  Scanning TLG00072.bin...\n",
      "    -> Found 575 points.\n",
      "  Scanning TLG00073.bin...\n",
      "    -> Found 5486 points.\n",
      "  Scanning TLG00074.bin...\n",
      "    -> Found 7584 points.\n",
      "  Scanning TLG00075.bin...\n",
      "    -> Found 2545 points.\n",
      "  Scanning TLG00076.bin...\n",
      "    -> Found 1015 points.\n",
      "  Scanning TLG00077.bin...\n",
      "    -> Found 2921 points.\n",
      "  Scanning TLG00078.bin...\n",
      "    -> Found 2976 points.\n",
      "  Scanning TLG00079.bin...\n",
      "    -> Found 3197 points.\n",
      "  Scanning TLG00080.bin...\n",
      "    -> Found 1164 points.\n",
      "  Scanning TLG00081.bin...\n",
      "    -> Found 2692 points.\n",
      "  Scanning TLG00082.bin...\n",
      "    -> Found 212 points.\n",
      "  Scanning TLG00083.bin...\n",
      "    -> Found 366 points.\n",
      "  Scanning TLG00084.bin...\n",
      "    -> Found 6 points.\n",
      "  Scanning TLG00085.bin...\n",
      "    -> Found 1994 points.\n",
      "  Scanning TLG00086.bin...\n",
      "    -> Found 2142 points.\n",
      "  Scanning TLG00087.bin...\n",
      "    -> Found 2409 points.\n",
      "  Scanning TLG00088.bin...\n",
      "    -> Found 2840 points.\n",
      "  Scanning TLG00089.bin...\n",
      "    -> Found 1713 points.\n",
      "  Scanning TLG00090.bin...\n",
      "    -> Found 460 points.\n",
      "  Scanning TLG00091.bin...\n",
      "    -> Found 295 points.\n",
      "  Scanning TLG00092.bin...\n",
      "    -> Found 1673 points.\n",
      "  Scanning TLG00093.bin...\n",
      "    -> Found 687 points.\n",
      "  Scanning TLG00094.bin...\n",
      "    -> Found 865 points.\n",
      "  Scanning TLG00095.bin...\n",
      "    -> Found 427 points.\n",
      "  Scanning TLG00096.bin...\n",
      "    -> Found 489 points.\n",
      "  Scanning TLG00097.bin...\n",
      "    -> Found 163 points.\n",
      "  Scanning TLG00098.bin...\n",
      "    -> Found 410 points.\n",
      "  Scanning TLG00099.bin...\n",
      "    -> Found 579 points.\n",
      "  Scanning TLG00100.bin...\n",
      "    -> Found 498 points.\n",
      "  Scanning TLG00101.bin...\n",
      "    -> Found 449 points.\n",
      "  Scanning TLG00102.bin...\n",
      "    -> Found 575 points.\n",
      "  Scanning TLG00103.bin...\n",
      "    -> Found 1514 points.\n",
      "  Scanning TLG00104.bin...\n",
      "    -> Found 1428 points.\n",
      "  Scanning TLG00105.bin...\n",
      "    -> Found 374 points.\n",
      "  Scanning TLG00106.bin...\n",
      "    -> Found 356 points.\n",
      "  Scanning TLG00107.bin...\n",
      "    -> Found 491 points.\n",
      "  Scanning TLG00108.bin...\n",
      "    -> Found 388 points.\n",
      "  Scanning TLG00109.bin...\n",
      "    -> Found 1509 points.\n",
      "  Scanning TLG00110.bin...\n",
      "    -> Found 1272 points.\n",
      "  Scanning TLG00111.bin...\n",
      "    -> Found 598 points.\n",
      "  Scanning TLG00112.bin...\n",
      "    -> Found 660 points.\n",
      "  Scanning TLG00113.bin...\n",
      "    -> Found 6 points.\n",
      "  Scanning TLG00114.bin...\n",
      "    -> Found 476 points.\n",
      "  Scanning TLG00115.bin...\n",
      "    -> Found 1276 points.\n",
      "  Scanning TLG00116.bin...\n",
      "    -> Found 1103 points.\n",
      "  Scanning TLG00117.bin...\n",
      "    -> Found 411 points.\n",
      "  Scanning TLG00118.bin...\n",
      "    -> Found 421 points.\n",
      "  Scanning TLG00119.bin...\n",
      "    -> Found 1293 points.\n",
      "  Scanning TLG00120.bin...\n",
      "    -> Found 2807 points.\n",
      "  Scanning TLG00121.bin...\n",
      "    -> Found 759 points.\n",
      "  Scanning TLG00122.bin...\n",
      "    -> Found 934 points.\n",
      "  Scanning TLG00123.bin...\n",
      "    -> Found 1430 points.\n",
      "  Scanning TLG00124.bin...\n",
      "    -> Found 582 points.\n",
      "  Scanning TLG00125.bin...\n",
      "    -> Found 666 points.\n",
      "  Scanning TLG00126.bin...\n",
      "    -> Found 128 points.\n",
      "  Scanning TLG00127.bin...\n",
      "    -> Found 114 points.\n",
      "  Scanning TLG00128.bin...\n",
      "    -> Found 173 points.\n",
      "  Scanning TLG00129.bin...\n",
      "    -> Found 139 points.\n",
      "  Scanning TLG00130.bin...\n",
      "    -> Found 359 points.\n",
      "  Scanning TLG00131.bin...\n",
      "    -> Found 621 points.\n",
      "  Scanning TLG00132.bin...\n",
      "    -> Found 131 points.\n",
      "  Scanning TLG00133.bin...\n",
      "    -> Found 1100 points.\n",
      "  Scanning TLG00134.bin...\n",
      "    -> Found 1852 points.\n",
      "  Scanning TLG00135.bin...\n",
      "    -> Found 1124 points.\n",
      "  Scanning TLG00136.bin...\n",
      "    -> Found 1173 points.\n",
      "  Scanning TLG00137.bin...\n",
      "    -> Found 289 points.\n",
      "  Scanning TLG00138.bin...\n",
      "    -> Found 1543 points.\n",
      "  Scanning TLG00139.bin...\n",
      "    -> Found 3673 points.\n",
      "  Scanning TLG00140.bin...\n",
      "    -> Found 1519 points.\n",
      "  Scanning TLG00141.bin...\n",
      "    -> Found 1518 points.\n",
      "  Scanning TLG00142.bin...\n",
      "    -> Found 978 points.\n",
      "\n",
      "SUCCESS! Extracted 182318 raw points.\n",
      "Saved to: ./data/taskdata_out\\BLIND_RAW_DATA.csv\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "data_folder = r'./data/taskdata'  # Path to your TLG files\n",
    "out_folder = r'./data/taskdata_out'\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "def get_dlv_defs(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        return [elem.attrib.get('A') for elem in tree.findall(\".//DLV\")]\n",
    "    except: return []\n",
    "\n",
    "def parse_blindly(bin_path, definitions):\n",
    "    \"\"\"\n",
    "    Scans file byte-by-byte. Accepts ANY valid coordinate (-90 to 90).\n",
    "    \"\"\"\n",
    "    with open(bin_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    total_len = len(content)\n",
    "    cursor = 0\n",
    "    valid_rows = []\n",
    "    \n",
    "    # ISO 11783-10 Type 4 Header: Time(4) + Lat(8) + Lon(8) + Status(1) + Count(1)\n",
    "    HEADER_SIZE = 22\n",
    "    \n",
    "    print(f\"  Scanning {os.path.basename(bin_path)}...\")\n",
    "\n",
    "    while cursor < total_len - HEADER_SIZE:\n",
    "        try:\n",
    "            # 1. Try to decode header\n",
    "            time_ms, lat_raw, lon_raw, status, count = struct.unpack('<IqqBB', content[cursor:cursor+HEADER_SIZE])\n",
    "            \n",
    "            # 2. Check Type 4 Scaling (1e-16)\n",
    "            lat = lat_raw * 1e-16\n",
    "            lon = lon_raw * 1e-16\n",
    "            \n",
    "            # 3. SANITY CHECK (Is it a coordinate on Earth?)\n",
    "            # We accept \"Russian\" data (Lat 60) and \"Danish\" data (Lat 55).\n",
    "            # We reject pure noise (Lat 0 or > 90).\n",
    "            if abs(lat) > 1 and abs(lat) <= 90 and abs(lon) > 1 and abs(lon) <= 180 and count < 50:\n",
    "                \n",
    "                # We found a valid frame!\n",
    "                payload_len = count + (count * 4)\n",
    "                if cursor + HEADER_SIZE + payload_len > total_len: break\n",
    "                \n",
    "                chunk = content[cursor + HEADER_SIZE : cursor + HEADER_SIZE + payload_len]\n",
    "                indices = struct.unpack(f\"{count}B\", chunk[:count])\n",
    "                values = struct.unpack(f\"{count}i\", chunk[count:])\n",
    "                \n",
    "                row = {\n",
    "                    'Time_ms': time_ms, \n",
    "                    'Latitude': lat, \n",
    "                    'Longitude': lon, \n",
    "                    'File': os.path.basename(bin_path)\n",
    "                }\n",
    "                \n",
    "                # Map DDI values\n",
    "                for i, idx in enumerate(indices):\n",
    "                    if idx < len(definitions):\n",
    "                        row[definitions[idx]] = values[i]\n",
    "                \n",
    "                valid_rows.append(row)\n",
    "                \n",
    "                # Jump forward by the full packet size\n",
    "                cursor += HEADER_SIZE + payload_len\n",
    "            else:\n",
    "                # Garbage/Header/Offset -> Shift 1 byte and try again\n",
    "                cursor += 1\n",
    "                \n",
    "        except struct.error:\n",
    "            cursor += 1\n",
    "\n",
    "    return pd.DataFrame(valid_rows)\n",
    "\n",
    "# --- EXECUTION ---\n",
    "all_dfs = []\n",
    "ddi_lookup = {'0054': 'Yield_Mass', '0053': 'Yield_Vol', '018D': 'Speed', '0063': 'Moisture', '0074': 'Area'}\n",
    "\n",
    "print(\"Starting Blind Scanner...\")\n",
    "\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.upper().startswith(\"TLG\") and filename.upper().endswith(\".BIN\"):\n",
    "        xml_full = os.path.join(data_folder, filename.rsplit('.', 1)[0] + \".xml\")\n",
    "        bin_full = os.path.join(data_folder, filename)\n",
    "        \n",
    "        if os.path.exists(xml_full):\n",
    "            defs = get_dlv_defs(xml_full)\n",
    "            df = parse_blindly(bin_full, defs)\n",
    "            \n",
    "            if not df.empty:\n",
    "                print(f\"    -> Found {len(df)} points.\")\n",
    "                all_dfs.append(df)\n",
    "\n",
    "if all_dfs:\n",
    "    master = pd.concat(all_dfs, ignore_index=True)\n",
    "    master.rename(columns=ddi_lookup, inplace=True)\n",
    "    \n",
    "    out_path = os.path.join(out_folder, 'BLIND_RAW_DATA.csv')\n",
    "    master.to_csv(out_path, index=False)\n",
    "    print(f\"\\nSUCCESS! Extracted {len(master)} raw points.\")\n",
    "    print(f\"Saved to: {out_path}\")\n",
    "else:\n",
    "    print(\"No data found even with blind scanning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd1521f7-151c-4f2b-84fc-c0da0dc4cff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Forensic Scan (Target: Ledreborg 55.5-55.75N)...\n",
      "Forensic scan of: TLG00001.bin...\n",
      "  -> SUCCESS: Recovered 205 points from TLG00001.bin\n",
      "Forensic scan of: TLG00002.bin...\n",
      "  -> SUCCESS: Recovered 275 points from TLG00002.bin\n",
      "Forensic scan of: TLG00003.bin...\n",
      "  -> SUCCESS: Recovered 2349 points from TLG00003.bin\n",
      "Forensic scan of: TLG00004.bin...\n",
      "  -> SUCCESS: Recovered 2753 points from TLG00004.bin\n",
      "Forensic scan of: TLG00005.bin...\n",
      "  -> SUCCESS: Recovered 300 points from TLG00005.bin\n",
      "Forensic scan of: TLG00006.bin...\n",
      "  -> SUCCESS: Recovered 185 points from TLG00006.bin\n",
      "Forensic scan of: TLG00007.bin...\n",
      "  -> SUCCESS: Recovered 4390 points from TLG00007.bin\n",
      "Forensic scan of: TLG00008.bin...\n",
      "  -> SUCCESS: Recovered 4782 points from TLG00008.bin\n",
      "Forensic scan of: TLG00009.bin...\n",
      "  -> SUCCESS: Recovered 4213 points from TLG00009.bin\n",
      "Forensic scan of: TLG00010.bin...\n",
      "  -> SUCCESS: Recovered 151 points from TLG00010.bin\n",
      "Forensic scan of: TLG00011.bin...\n",
      "  -> SUCCESS: Recovered 367 points from TLG00011.bin\n",
      "Forensic scan of: TLG00012.bin...\n",
      "  -> SUCCESS: Recovered 336 points from TLG00012.bin\n",
      "Forensic scan of: TLG00013.bin...\n",
      "  -> SUCCESS: Recovered 12 points from TLG00013.bin\n",
      "Forensic scan of: TLG00014.bin...\n",
      "  -> SUCCESS: Recovered 150 points from TLG00014.bin\n",
      "Forensic scan of: TLG00015.bin...\n",
      "  -> SUCCESS: Recovered 153 points from TLG00015.bin\n",
      "Forensic scan of: TLG00016.bin...\n",
      "  -> SUCCESS: Recovered 301 points from TLG00016.bin\n",
      "Forensic scan of: TLG00017.bin...\n",
      "  -> SUCCESS: Recovered 284 points from TLG00017.bin\n",
      "Forensic scan of: TLG00018.bin...\n",
      "  -> SUCCESS: Recovered 779 points from TLG00018.bin\n",
      "Forensic scan of: TLG00019.bin...\n",
      "  -> SUCCESS: Recovered 1322 points from TLG00019.bin\n",
      "Forensic scan of: TLG00020.bin...\n",
      "  -> SUCCESS: Recovered 1287 points from TLG00020.bin\n",
      "Forensic scan of: TLG00021.bin...\n",
      "  -> SUCCESS: Recovered 910 points from TLG00021.bin\n",
      "Forensic scan of: TLG00022.bin...\n",
      "  -> SUCCESS: Recovered 863 points from TLG00022.bin\n",
      "Forensic scan of: TLG00023.bin...\n",
      "  -> SUCCESS: Recovered 613 points from TLG00023.bin\n",
      "Forensic scan of: TLG00024.bin...\n",
      "  -> SUCCESS: Recovered 448 points from TLG00024.bin\n",
      "Forensic scan of: TLG00025.bin...\n",
      "  -> SUCCESS: Recovered 955 points from TLG00025.bin\n",
      "Forensic scan of: TLG00026.bin...\n",
      "  -> SUCCESS: Recovered 906 points from TLG00026.bin\n",
      "Forensic scan of: TLG00027.bin...\n",
      "  -> SUCCESS: Recovered 400 points from TLG00027.bin\n",
      "Forensic scan of: TLG00028.bin...\n",
      "  -> SUCCESS: Recovered 382 points from TLG00028.bin\n",
      "Forensic scan of: TLG00029.bin...\n",
      "  -> SUCCESS: Recovered 799 points from TLG00029.bin\n",
      "Forensic scan of: TLG00030.bin...\n",
      "  -> SUCCESS: Recovered 282 points from TLG00030.bin\n",
      "Forensic scan of: TLG00031.bin...\n",
      "  -> SUCCESS: Recovered 248 points from TLG00031.bin\n",
      "Forensic scan of: TLG00032.bin...\n",
      "  -> SUCCESS: Recovered 112 points from TLG00032.bin\n",
      "Forensic scan of: TLG00033.bin...\n",
      "  -> SUCCESS: Recovered 340 points from TLG00033.bin\n",
      "Forensic scan of: TLG00034.bin...\n",
      "  -> SUCCESS: Recovered 3204 points from TLG00034.bin\n",
      "Forensic scan of: TLG00035.bin...\n",
      "  -> SUCCESS: Recovered 2811 points from TLG00035.bin\n",
      "Forensic scan of: TLG00036.bin...\n",
      "  -> SUCCESS: Recovered 123 points from TLG00036.bin\n",
      "Forensic scan of: TLG00037.bin...\n",
      "  -> SUCCESS: Recovered 59 points from TLG00037.bin\n",
      "Forensic scan of: TLG00038.bin...\n",
      "  -> SUCCESS: Recovered 4666 points from TLG00038.bin\n",
      "Forensic scan of: TLG00039.bin...\n",
      "  -> SUCCESS: Recovered 280 points from TLG00039.bin\n",
      "Forensic scan of: TLG00040.bin...\n",
      "  -> SUCCESS: Recovered 281 points from TLG00040.bin\n",
      "Forensic scan of: TLG00041.bin...\n",
      "  -> SUCCESS: Recovered 949 points from TLG00041.bin\n",
      "Forensic scan of: TLG00042.bin...\n",
      "  -> SUCCESS: Recovered 1033 points from TLG00042.bin\n",
      "Forensic scan of: TLG00043.bin...\n",
      "  -> SUCCESS: Recovered 3174 points from TLG00043.bin\n",
      "Forensic scan of: TLG00044.bin...\n",
      "  -> SUCCESS: Recovered 2774 points from TLG00044.bin\n",
      "Forensic scan of: TLG00045.bin...\n",
      "  -> SUCCESS: Recovered 135 points from TLG00045.bin\n",
      "Forensic scan of: TLG00046.bin...\n",
      "  -> SUCCESS: Recovered 168 points from TLG00046.bin\n",
      "Forensic scan of: TLG00047.bin...\n",
      "  -> SUCCESS: Recovered 689 points from TLG00047.bin\n",
      "Forensic scan of: TLG00048.bin...\n",
      "  -> SUCCESS: Recovered 419 points from TLG00048.bin\n",
      "Forensic scan of: TLG00049.bin...\n",
      "  -> SUCCESS: Recovered 3633 points from TLG00049.bin\n",
      "Forensic scan of: TLG00050.bin...\n",
      "  -> SUCCESS: Recovered 3683 points from TLG00050.bin\n",
      "Forensic scan of: TLG00051.bin...\n",
      "  -> SUCCESS: Recovered 924 points from TLG00051.bin\n",
      "Forensic scan of: TLG00052.bin...\n",
      "  -> SUCCESS: Recovered 1000 points from TLG00052.bin\n",
      "Forensic scan of: TLG00053.bin...\n",
      "  -> SUCCESS: Recovered 183 points from TLG00053.bin\n",
      "Forensic scan of: TLG00054.bin...\n",
      "  -> SUCCESS: Recovered 172 points from TLG00054.bin\n",
      "Forensic scan of: TLG00055.bin...\n",
      "  -> SUCCESS: Recovered 855 points from TLG00055.bin\n",
      "Forensic scan of: TLG00056.bin...\n",
      "  -> SUCCESS: Recovered 762 points from TLG00056.bin\n",
      "Forensic scan of: TLG00057.bin...\n",
      "  -> SUCCESS: Recovered 422 points from TLG00057.bin\n",
      "Forensic scan of: TLG00058.bin...\n",
      "  -> SUCCESS: Recovered 277 points from TLG00058.bin\n",
      "Forensic scan of: TLG00059.bin...\n",
      "  -> SUCCESS: Recovered 232 points from TLG00059.bin\n",
      "Forensic scan of: TLG00060.bin...\n",
      "  -> SUCCESS: Recovered 288 points from TLG00060.bin\n",
      "Forensic scan of: TLG00061.bin...\n",
      "  -> SUCCESS: Recovered 1396 points from TLG00061.bin\n",
      "Forensic scan of: TLG00062.bin...\n",
      "  -> SUCCESS: Recovered 425 points from TLG00062.bin\n",
      "Forensic scan of: TLG00063.bin...\n",
      "  -> SUCCESS: Recovered 957 points from TLG00063.bin\n",
      "Forensic scan of: TLG00064.bin...\n",
      "  -> SUCCESS: Recovered 833 points from TLG00064.bin\n",
      "Forensic scan of: TLG00065.bin...\n",
      "  -> SUCCESS: Recovered 62 points from TLG00065.bin\n",
      "Forensic scan of: TLG00066.bin...\n",
      "  -> SUCCESS: Recovered 134 points from TLG00066.bin\n",
      "Forensic scan of: TLG00067.bin...\n",
      "  -> SUCCESS: Recovered 221 points from TLG00067.bin\n",
      "Forensic scan of: TLG00068.bin...\n",
      "  -> SUCCESS: Recovered 1308 points from TLG00068.bin\n",
      "Forensic scan of: TLG00069.bin...\n",
      "  -> SUCCESS: Recovered 1315 points from TLG00069.bin\n",
      "Forensic scan of: TLG00070.bin...\n",
      "  -> SUCCESS: Recovered 193 points from TLG00070.bin\n",
      "Forensic scan of: TLG00071.bin...\n",
      "  -> SUCCESS: Recovered 620 points from TLG00071.bin\n",
      "Forensic scan of: TLG00072.bin...\n",
      "  -> SUCCESS: Recovered 344 points from TLG00072.bin\n",
      "Forensic scan of: TLG00073.bin...\n",
      "  -> SUCCESS: Recovered 3830 points from TLG00073.bin\n",
      "Forensic scan of: TLG00074.bin...\n",
      "  -> SUCCESS: Recovered 4444 points from TLG00074.bin\n",
      "Forensic scan of: TLG00075.bin...\n",
      "  -> SUCCESS: Recovered 966 points from TLG00075.bin\n",
      "Forensic scan of: TLG00076.bin...\n",
      "  -> SUCCESS: Recovered 686 points from TLG00076.bin\n",
      "Forensic scan of: TLG00077.bin...\n",
      "  -> SUCCESS: Recovered 1984 points from TLG00077.bin\n",
      "Forensic scan of: TLG00078.bin...\n",
      "  -> SUCCESS: Recovered 2601 points from TLG00078.bin\n",
      "Forensic scan of: TLG00079.bin...\n",
      "  -> SUCCESS: Recovered 1896 points from TLG00079.bin\n",
      "Forensic scan of: TLG00080.bin...\n",
      "  -> SUCCESS: Recovered 895 points from TLG00080.bin\n",
      "Forensic scan of: TLG00081.bin...\n",
      "  -> SUCCESS: Recovered 1660 points from TLG00081.bin\n",
      "Forensic scan of: TLG00082.bin...\n",
      "  -> SUCCESS: Recovered 140 points from TLG00082.bin\n",
      "Forensic scan of: TLG00083.bin...\n",
      "  -> SUCCESS: Recovered 361 points from TLG00083.bin\n",
      "Forensic scan of: TLG00084.bin...\n",
      "  -> SUCCESS: Recovered 2 points from TLG00084.bin\n",
      "Forensic scan of: TLG00085.bin...\n",
      "  -> SUCCESS: Recovered 1893 points from TLG00085.bin\n",
      "Forensic scan of: TLG00086.bin...\n",
      "  -> SUCCESS: Recovered 1283 points from TLG00086.bin\n",
      "Forensic scan of: TLG00087.bin...\n",
      "  -> SUCCESS: Recovered 1594 points from TLG00087.bin\n",
      "Forensic scan of: TLG00088.bin...\n",
      "  -> SUCCESS: Recovered 1572 points from TLG00088.bin\n",
      "Forensic scan of: TLG00089.bin...\n",
      "  -> SUCCESS: Recovered 1002 points from TLG00089.bin\n",
      "Forensic scan of: TLG00090.bin...\n",
      "  -> SUCCESS: Recovered 374 points from TLG00090.bin\n",
      "Forensic scan of: TLG00091.bin...\n",
      "  -> SUCCESS: Recovered 234 points from TLG00091.bin\n",
      "Forensic scan of: TLG00092.bin...\n",
      "  -> SUCCESS: Recovered 1566 points from TLG00092.bin\n",
      "Forensic scan of: TLG00093.bin...\n",
      "  -> SUCCESS: Recovered 773 points from TLG00093.bin\n",
      "Forensic scan of: TLG00094.bin...\n",
      "  -> SUCCESS: Recovered 812 points from TLG00094.bin\n",
      "Forensic scan of: TLG00095.bin...\n",
      "  -> SUCCESS: Recovered 356 points from TLG00095.bin\n",
      "Forensic scan of: TLG00096.bin...\n",
      "  -> SUCCESS: Recovered 392 points from TLG00096.bin\n",
      "Forensic scan of: TLG00097.bin...\n",
      "  -> SUCCESS: Recovered 78 points from TLG00097.bin\n",
      "Forensic scan of: TLG00098.bin...\n",
      "  -> SUCCESS: Recovered 250 points from TLG00098.bin\n",
      "Forensic scan of: TLG00099.bin...\n",
      "  -> SUCCESS: Recovered 396 points from TLG00099.bin\n",
      "Forensic scan of: TLG00100.bin...\n",
      "  -> SUCCESS: Recovered 408 points from TLG00100.bin\n",
      "Forensic scan of: TLG00101.bin...\n",
      "  -> SUCCESS: Recovered 469 points from TLG00101.bin\n",
      "Forensic scan of: TLG00102.bin...\n",
      "  -> SUCCESS: Recovered 496 points from TLG00102.bin\n",
      "Forensic scan of: TLG00103.bin...\n",
      "  -> SUCCESS: Recovered 1393 points from TLG00103.bin\n",
      "Forensic scan of: TLG00104.bin...\n",
      "  -> SUCCESS: Recovered 1349 points from TLG00104.bin\n",
      "Forensic scan of: TLG00105.bin...\n",
      "  -> SUCCESS: Recovered 304 points from TLG00105.bin\n",
      "Forensic scan of: TLG00106.bin...\n",
      "  -> SUCCESS: Recovered 316 points from TLG00106.bin\n",
      "Forensic scan of: TLG00107.bin...\n",
      "  -> SUCCESS: Recovered 268 points from TLG00107.bin\n",
      "Forensic scan of: TLG00108.bin...\n",
      "  -> SUCCESS: Recovered 255 points from TLG00108.bin\n",
      "Forensic scan of: TLG00109.bin...\n",
      "  -> SUCCESS: Recovered 1045 points from TLG00109.bin\n",
      "Forensic scan of: TLG00110.bin...\n",
      "  -> SUCCESS: Recovered 1020 points from TLG00110.bin\n",
      "Forensic scan of: TLG00111.bin...\n",
      "  -> SUCCESS: Recovered 583 points from TLG00111.bin\n",
      "Forensic scan of: TLG00112.bin...\n",
      "  -> SUCCESS: Recovered 614 points from TLG00112.bin\n",
      "Forensic scan of: TLG00113.bin...\n",
      "  -> SUCCESS: Recovered 9 points from TLG00113.bin\n",
      "Forensic scan of: TLG00114.bin...\n",
      "  -> SUCCESS: Recovered 344 points from TLG00114.bin\n",
      "Forensic scan of: TLG00115.bin...\n",
      "  -> SUCCESS: Recovered 638 points from TLG00115.bin\n",
      "Forensic scan of: TLG00116.bin...\n",
      "  -> SUCCESS: Recovered 534 points from TLG00116.bin\n",
      "Forensic scan of: TLG00117.bin...\n",
      "  -> SUCCESS: Recovered 302 points from TLG00117.bin\n",
      "Forensic scan of: TLG00118.bin...\n",
      "  -> SUCCESS: Recovered 322 points from TLG00118.bin\n",
      "Forensic scan of: TLG00119.bin...\n",
      "  -> SUCCESS: Recovered 1316 points from TLG00119.bin\n",
      "Forensic scan of: TLG00120.bin...\n",
      "  -> SUCCESS: Recovered 1782 points from TLG00120.bin\n",
      "Forensic scan of: TLG00121.bin...\n",
      "  -> SUCCESS: Recovered 447 points from TLG00121.bin\n",
      "Forensic scan of: TLG00122.bin...\n",
      "  -> SUCCESS: Recovered 614 points from TLG00122.bin\n",
      "Forensic scan of: TLG00123.bin...\n",
      "  -> SUCCESS: Recovered 1128 points from TLG00123.bin\n",
      "Forensic scan of: TLG00124.bin...\n",
      "  -> SUCCESS: Recovered 605 points from TLG00124.bin\n",
      "Forensic scan of: TLG00125.bin...\n",
      "  -> SUCCESS: Recovered 656 points from TLG00125.bin\n",
      "Forensic scan of: TLG00126.bin...\n",
      "  -> SUCCESS: Recovered 92 points from TLG00126.bin\n",
      "Forensic scan of: TLG00127.bin...\n",
      "  -> SUCCESS: Recovered 76 points from TLG00127.bin\n",
      "Forensic scan of: TLG00128.bin...\n",
      "  -> SUCCESS: Recovered 159 points from TLG00128.bin\n",
      "Forensic scan of: TLG00129.bin...\n",
      "  -> SUCCESS: Recovered 144 points from TLG00129.bin\n",
      "Forensic scan of: TLG00130.bin...\n",
      "  -> SUCCESS: Recovered 231 points from TLG00130.bin\n",
      "Forensic scan of: TLG00131.bin...\n",
      "  -> SUCCESS: Recovered 483 points from TLG00131.bin\n",
      "Forensic scan of: TLG00132.bin...\n",
      "  -> SUCCESS: Recovered 99 points from TLG00132.bin\n",
      "Forensic scan of: TLG00133.bin...\n",
      "  -> SUCCESS: Recovered 963 points from TLG00133.bin\n",
      "Forensic scan of: TLG00134.bin...\n",
      "  -> SUCCESS: Recovered 1670 points from TLG00134.bin\n",
      "Forensic scan of: TLG00135.bin...\n",
      "  -> SUCCESS: Recovered 1059 points from TLG00135.bin\n",
      "Forensic scan of: TLG00136.bin...\n",
      "  -> SUCCESS: Recovered 1058 points from TLG00136.bin\n",
      "Forensic scan of: TLG00137.bin...\n",
      "  -> SUCCESS: Recovered 229 points from TLG00137.bin\n",
      "Forensic scan of: TLG00138.bin...\n",
      "  -> SUCCESS: Recovered 1023 points from TLG00138.bin\n",
      "Forensic scan of: TLG00139.bin...\n",
      "  -> SUCCESS: Recovered 2642 points from TLG00139.bin\n",
      "Forensic scan of: TLG00140.bin...\n",
      "  -> SUCCESS: Recovered 855 points from TLG00140.bin\n",
      "Forensic scan of: TLG00141.bin...\n",
      "  -> SUCCESS: Recovered 1027 points from TLG00141.bin\n",
      "Forensic scan of: TLG00142.bin...\n",
      "  -> SUCCESS: Recovered 732 points from TLG00142.bin\n",
      "\n",
      "DONE. Saved 134655 clean points to: ./data/taskdata_out\\FORENSIC_HARVEST_DATA.csv\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "data_folder = r'./data/taskdata'\n",
    "out_folder = r'./data/taskdata_out'\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "# THE TARGET: Ledreborg Gods (Strict Filter)\n",
    "# We only accept data that falls exactly inside this box.\n",
    "TARGET_LAT_MIN, TARGET_LAT_MAX = 55.50, 55.75\n",
    "TARGET_LON_MIN, TARGET_LON_MAX = 11.80, 12.10\n",
    "\n",
    "def get_dlv_defs(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        return [elem.attrib.get('A') for elem in tree.findall(\".//DLV\")]\n",
    "    except: return []\n",
    "\n",
    "def brute_force_recover(bin_path, definitions):\n",
    "    print(f\"Forensic scan of: {os.path.basename(bin_path)}...\")\n",
    "    \n",
    "    with open(bin_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    total_len = len(content)\n",
    "    cursor = 0\n",
    "    valid_rows = []\n",
    "    \n",
    "    # We define the column names mapping here\n",
    "    ddi_lookup = {\n",
    "        '0054': 'Yield_Mass', \n",
    "        '0053': 'Yield_Vol', \n",
    "        '018D': 'Speed', \n",
    "        '0063': 'Moisture', \n",
    "        '0074': 'Area',\n",
    "        '0106': 'Moisture_Alt'\n",
    "    }\n",
    "\n",
    "    while cursor < total_len - 50: # Need at least 50 bytes for a row\n",
    "        match_found = False\n",
    "        \n",
    "        # --- ATTEMPT 1: TYPE 2 (64-bit, 1e-16 scaling) ---\n",
    "        # Header: Time(4) + Lat(8) + Lon(8) + Status(1) + Count(1) = 22 bytes\n",
    "        try:\n",
    "            time_ms, lat_raw, lon_raw, status, count = struct.unpack('<IqqBB', content[cursor:cursor+22])\n",
    "            \n",
    "            lat = lat_raw * 1e-16\n",
    "            lon = lon_raw * 1e-16\n",
    "            \n",
    "            if (TARGET_LAT_MIN < lat < TARGET_LAT_MAX) and (TARGET_LON_MIN < lon < TARGET_LON_MAX):\n",
    "                # HIT! We found a Type 2 row in Ledreborg\n",
    "                payload_len = count + (count * 4)\n",
    "                \n",
    "                # Check if payload fits in file\n",
    "                if cursor + 22 + payload_len <= total_len:\n",
    "                    chunk = content[cursor + 22 : cursor + 22 + payload_len]\n",
    "                    indices = struct.unpack(f\"{count}B\", chunk[:count])\n",
    "                    values = struct.unpack(f\"{count}i\", chunk[count:])\n",
    "                    \n",
    "                    row = {'Time_ms': time_ms, 'Latitude': lat, 'Longitude': lon, 'Type': 'Type2'}\n",
    "                    for i, idx in enumerate(indices):\n",
    "                        if idx < len(definitions):\n",
    "                            ddi = definitions[idx]\n",
    "                            row[ddi_lookup.get(ddi, f'DDI_{ddi}')] = values[i]\n",
    "                    \n",
    "                    valid_rows.append(row)\n",
    "                    cursor += 22 + payload_len\n",
    "                    match_found = True\n",
    "        except: pass\n",
    "\n",
    "        if match_found: continue\n",
    "\n",
    "        # --- ATTEMPT 2: TYPE 1 (32-bit, 1e-7 scaling) ---\n",
    "        # Header: Time(4) + Lat(4) + Lon(4) = 12 bytes\n",
    "        # Note: Type 1 is harder because it doesn't have a \"Count\" byte. \n",
    "        # We assume the number of DLVs matches the XML definition exactly.\n",
    "        try:\n",
    "            time_ms, lat_raw, lon_raw = struct.unpack('<Lii', content[cursor:cursor+12])\n",
    "            \n",
    "            lat = lat_raw * 1e-7\n",
    "            lon = lon_raw * 1e-7\n",
    "            \n",
    "            if (TARGET_LAT_MIN < lat < TARGET_LAT_MAX) and (TARGET_LON_MIN < lon < TARGET_LON_MAX):\n",
    "                # HIT! We found a Type 1 row in Ledreborg\n",
    "                \n",
    "                # Assume standard payload: all DLVs defined in XML are present as 4-byte ints\n",
    "                count = len(definitions)\n",
    "                payload_len = count * 4\n",
    "                \n",
    "                if cursor + 12 + payload_len <= total_len:\n",
    "                    values = struct.unpack(f\"{count}i\", content[cursor+12 : cursor+12+payload_len])\n",
    "                    \n",
    "                    row = {'Time_ms': time_ms, 'Latitude': lat, 'Longitude': lon, 'Type': 'Type1'}\n",
    "                    for i, val in enumerate(values):\n",
    "                        ddi = definitions[i]\n",
    "                        row[ddi_lookup.get(ddi, f'DDI_{ddi}')] = val\n",
    "                    \n",
    "                    valid_rows.append(row)\n",
    "                    cursor += 12 + payload_len\n",
    "                    match_found = True\n",
    "        except: pass\n",
    "\n",
    "        if match_found: continue\n",
    "        \n",
    "        # If no match, shift by ONE byte and try again\n",
    "        cursor += 1\n",
    "\n",
    "    return pd.DataFrame(valid_rows)\n",
    "\n",
    "# --- EXECUTION ---\n",
    "all_dfs = []\n",
    "print(f\"Starting Forensic Scan (Target: Ledreborg {TARGET_LAT_MIN}-{TARGET_LAT_MAX}N)...\")\n",
    "\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.upper().startswith(\"TLG\") and filename.upper().endswith(\".BIN\"):\n",
    "        xml_full = os.path.join(data_folder, filename.rsplit('.', 1)[0] + \".xml\")\n",
    "        bin_full = os.path.join(data_folder, filename)\n",
    "        \n",
    "        if os.path.exists(xml_full):\n",
    "            defs = get_dlv_defs(xml_full)\n",
    "            df = brute_force_recover(bin_full, defs)\n",
    "            \n",
    "            if not df.empty:\n",
    "                df['File'] = filename\n",
    "                print(f\"  -> SUCCESS: Recovered {len(df)} points from {filename}\")\n",
    "                all_dfs.append(df)\n",
    "            else:\n",
    "                print(f\"  -> No valid Ledreborg data found in {filename}\")\n",
    "\n",
    "if all_dfs:\n",
    "    master = pd.concat(all_dfs, ignore_index=True)\n",
    "    out_path = os.path.join(out_folder, 'FORENSIC_HARVEST_DATA.csv')\n",
    "    master.to_csv(out_path, index=False)\n",
    "    print(f\"\\nDONE. Saved {len(master)} clean points to: {out_path}\")\n",
    "else:\n",
    "    print(\"\\nFAILED. Could not find any coordinates inside Ledreborg.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c41c67dd-e5c3-44bc-bbd9-33b33de8f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Parsing Task Start Times...\n",
      "Calculating Datetime...\n",
      "Cleaning Noise...\n",
      "Recalculating Speed from GPS...\n",
      "Success! Saved 8 clean points to ./data/taskdata_out/CLEANED_HARVEST_WITH_DATE.csv\n",
      "Columns: Datetime, Latitude, Longitude, Yield_kg_s, Speed_Smooth (m/s)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "input_csv = './data/taskdata_out/FORENSIC_HARVEST_DATA.csv'\n",
    "taskdata_xml = './data/taskdata/TASKDATA.XML'\n",
    "output_csv = './data/taskdata_out/CLEANED_HARVEST_WITH_DATE.csv'\n",
    "\n",
    "# --- 1. LOAD DATA & DATES ---\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(input_csv)\n",
    "df['Time_ms'] = pd.to_numeric(df['Time_ms'], errors='coerce')\n",
    "\n",
    "# Parse Task Start Times from XML\n",
    "print(\"Parsing Task Start Times...\")\n",
    "tree = ET.parse(taskdata_xml)\n",
    "root = tree.getroot()\n",
    "\n",
    "tlg_map = {}\n",
    "for tsk in root.findall(\".//TSK\"):\n",
    "    tlg = tsk.find(\"TLG\")\n",
    "    tim = tsk.find(\"TIM\")\n",
    "    if tlg is not None and tim is not None:\n",
    "        fname = tlg.attrib.get('A')\n",
    "        start = tim.attrib.get('A')\n",
    "        if fname and start:\n",
    "            # Map both \"TLG00001\" and \"TLG00001.bin\" to be safe\n",
    "            tlg_map[fname] = start\n",
    "            tlg_map[fname + '.bin'] = start\n",
    "\n",
    "# --- 2. CALCULATE DATETIME ---\n",
    "print(\"Calculating Datetime...\")\n",
    "# Map the file to its start time\n",
    "df['TaskStart'] = df['File'].map(tlg_map)\n",
    "df['TaskStart'] = pd.to_datetime(df['TaskStart'])\n",
    "\n",
    "# Add the millisecond offset\n",
    "df['Datetime'] = df['TaskStart'] + pd.to_timedelta(df['Time_ms'], unit='ms')\n",
    "\n",
    "# --- 3. CLEANING & RECALCULATION ---\n",
    "print(\"Cleaning Noise...\")\n",
    "\n",
    "# A. Keep only rows with valid Yield (0 to 50 kg/s)\n",
    "# Raw Yield is mg/s. 50 kg/s = 50,000,000.\n",
    "df_clean = df[\n",
    "    (df['Yield_Mass'] > 0) & \n",
    "    (df['Yield_Mass'] < 50000000)\n",
    "].copy()\n",
    "\n",
    "# B. Deduplicate\n",
    "# The scanner found duplicates (same time). We keep the first valid one.\n",
    "df_clean = df_clean.drop_duplicates(subset=['File', 'Time_ms'])\n",
    "\n",
    "# C. Calculate GPS Speed (Fixes the \"Strange Effect\" from broken Speed sensor)\n",
    "print(\"Recalculating Speed from GPS...\")\n",
    "df_clean = df_clean.sort_values(['File', 'Time_ms'])\n",
    "\n",
    "# Calculate Distance (Pythagoras on Lat/Lon degrees -> meters)\n",
    "# 1 deg Lat ~= 111,132 m\n",
    "# 1 deg Lon ~= 111,132 * cos(lat) m\n",
    "lat_rad = np.radians(df_clean['Latitude'])\n",
    "dlat = df_clean.groupby('File')['Latitude'].diff() * 111132\n",
    "dlon = df_clean.groupby('File')['Longitude'].diff() * 111132 * np.cos(lat_rad)\n",
    "dist_m = np.sqrt(dlat**2 + dlon**2)\n",
    "\n",
    "# Calculate Time Delta (seconds)\n",
    "dt_s = df_clean.groupby('File')['Time_ms'].diff() / 1000.0\n",
    "\n",
    "# Speed = Dist / Time\n",
    "df_clean['Speed_GPS_m_s'] = dist_m / dt_s\n",
    "\n",
    "# Smooth Speed (Rolling average to remove GPS jitter)\n",
    "df_clean['Speed_Smooth'] = df_clean.groupby('File')['Speed_GPS_m_s'].transform(\n",
    "    lambda x: x.rolling(window=5, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# Filter out impossible speeds (e.g., > 20 m/s or stopped)\n",
    "df_final = df_clean[\n",
    "    (df_clean['Speed_Smooth'] > 0.5) & \n",
    "    (df_clean['Speed_Smooth'] < 20)\n",
    "].copy()\n",
    "\n",
    "# --- 4. FORMAT OUTPUT ---\n",
    "# Convert Yield to kg/s for readability\n",
    "df_final['Yield_kg_s'] = df_final['Yield_Mass'] / 1_000_000\n",
    "\n",
    "# Select useful columns\n",
    "out_cols = [\n",
    "    'Datetime', 'Latitude', 'Longitude', \n",
    "    'Yield_kg_s', 'Speed_Smooth', 'Moisture', 'File'\n",
    "]\n",
    "\n",
    "df_final[out_cols].to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Success! Saved {len(df_final)} clean points to {output_csv}\")\n",
    "print(\"Columns: Datetime, Latitude, Longitude, Yield_kg_s, Speed_Smooth (m/s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8f24f-2df7-4509-8800-84f92068db04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
