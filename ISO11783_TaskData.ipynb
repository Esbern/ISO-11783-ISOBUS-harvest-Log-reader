{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0390063-6d5f-4ed1-961c-6646859aca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1b5e02-4d35-4432-997a-b514707a80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soecifu the taskdata folder\n",
    "\n",
    "data_folder = r'./data/taskdata_3' \n",
    "out_folder = r'./data/taskdata_3_out'\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "# Buffer to add around the farm boundaries (in degrees)\n",
    "# 0.02 deg is approx 2km. This accounts for headlands/approach roads.\n",
    "GEO_BUFFER = 0.02 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e2a81-ca46-4243-a785-ac3fa4cba27b",
   "metadata": {},
   "source": [
    "## XML task file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e964cab8-18f1-4096-abd9-3498d463b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_isobus_taskdata(data_folder):\n",
    "    # 1. Find TASKDATA.XML\n",
    "    taskdata_path = None\n",
    "    for file in os.listdir(data_folder):\n",
    "        if file.upper() == 'TASKDATA.XML':\n",
    "            taskdata_path = os.path.join(data_folder, file)\n",
    "            break\n",
    "            \n",
    "    if taskdata_path is None:\n",
    "        print(\"Error: TASKDATA.XML not found.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        tree = ET.parse(taskdata_path)\n",
    "        root = tree.getroot()\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 1. Map Products (PDT) ---\n",
    "    products = {}\n",
    "    for pdt in root.findall(\".//PDT\"):\n",
    "        products[pdt.attrib.get('A')] = pdt.attrib.get('B')\n",
    "\n",
    "    # --- 2. Store Field Geometries (PFD) ---\n",
    "    field_shapes = {}   # PFD_ID -> Coordinates\n",
    "    field_names_map = {} # PFD_ID -> Name\n",
    "\n",
    "    for pfd in root.findall(\".//PFD\"):\n",
    "        pfd_id = pfd.attrib.get('A')\n",
    "        field_names_map[pfd_id] = pfd.attrib.get('C')\n",
    "        \n",
    "        # Extract Geometry (Polygon)\n",
    "        # We take the first valid polygon found\n",
    "        for pln in pfd.findall(\"PLN\"):\n",
    "            for lsg in pln.findall(\"LSG\"):\n",
    "                coordinates = []\n",
    "                for pnt in lsg.findall(\"PNT\"):\n",
    "                    try:\n",
    "                        # XML Attributes are strings, convert to float\n",
    "                        lat = float(pnt.attrib.get('C'))\n",
    "                        lon = float(pnt.attrib.get('D'))\n",
    "                        coordinates.append([lon, lat])\n",
    "                    except: continue\n",
    "                \n",
    "                if len(coordinates) > 2:\n",
    "                    # Close the polygon loop\n",
    "                    if coordinates[0] != coordinates[-1]:\n",
    "                        coordinates.append(coordinates[0])\n",
    "                    field_shapes[pfd_id] = coordinates\n",
    "                    break # Stop after first valid shape\n",
    "\n",
    "    # --- 3. Build Tasks & Generate GeoJSON ---\n",
    "    tasks_list = []\n",
    "    geojson_features = []\n",
    "\n",
    "    for tsk in root.findall(\".//TSK\"):\n",
    "        task_id = tsk.attrib.get('A')\n",
    "        field_ref = tsk.attrib.get('E')\n",
    "        \n",
    "        # Get Attributes\n",
    "        field_name = field_names_map.get(field_ref, f\"Unknown ({field_ref})\")\n",
    "        \n",
    "        # Get Log File\n",
    "        tlg = tsk.find(\"TLG\")\n",
    "        log_filename = tlg.attrib.get('A') if tlg is not None else None\n",
    "\n",
    "        # Get Crop\n",
    "        crop_name = \"Unknown\"\n",
    "        pan = tsk.find(\"PAN\")\n",
    "        if pan is not None:\n",
    "            pdt_ref = pan.attrib.get('A')\n",
    "            crop_name = products.get(pdt_ref, pdt_ref)\n",
    "            \n",
    "        # --- NEW: GET YEAR ---\n",
    "        # Find all TIM tags and get the earliest start time\n",
    "        start_times = []\n",
    "        for tim in tsk.findall(\"TIM\"):\n",
    "            start_str = tim.attrib.get('A') # Format: 2023-08-16T11:34:57...\n",
    "            if start_str:\n",
    "                try:\n",
    "                    # Parse ISO format (handle timezone if present, simplistically here)\n",
    "                    dt = datetime.fromisoformat(start_str.replace('Z', '+00:00'))\n",
    "                    start_times.append(dt)\n",
    "                except: pass\n",
    "        \n",
    "        task_year = start_times[0].year if start_times else None\n",
    "\n",
    "        # -- Build DataFrame Entry --\n",
    "        if log_filename: # Only save interesting tasks with logs\n",
    "            tasks_list.append({\n",
    "                'TaskID': task_id,\n",
    "                'Year': task_year,\n",
    "                'Crop': crop_name,\n",
    "                'FieldName': field_name,\n",
    "                'FieldID': field_ref,\n",
    "                'LogFilename': log_filename\n",
    "            })\n",
    "\n",
    "            # -- Build GeoJSON Feature --\n",
    "            if field_ref in field_shapes:\n",
    "                feature = {\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"properties\": {\n",
    "                        \"TaskID\": task_id,\n",
    "                        \"Year\": int(task_year) if task_year else None,\n",
    "                        \"FieldName\": field_name,\n",
    "                        \"Crop\": crop_name,\n",
    "                        \"LogFilename\": log_filename \n",
    "                    },\n",
    "                    \"geometry\": {\n",
    "                        \"type\": \"Polygon\",\n",
    "                        \"coordinates\": [field_shapes[field_ref]]\n",
    "                    }\n",
    "                }\n",
    "                geojson_features.append(feature)\n",
    "\n",
    "    # --- Output ---\n",
    "    geojson_output = {\n",
    "        \"type\": \"FeatureCollection\", \n",
    "        \"features\": geojson_features\n",
    "    }\n",
    "    df_tasks = pd.DataFrame(tasks_list)\n",
    "    \n",
    "    return geojson_output, df_tasks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b79b52-5df0-4395-9cc8-050ebdbdea9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. Index saved to ./data/taskdata_3_out\\task_index.csv\n",
      "Sample:\n",
      "   Year              Crop             FieldName\n",
      "0  2022      RG - Rajgræs         021-0, Monica\n",
      "1  2022      RG - Rajgræs         021-0, Monica\n",
      "2  2021  RA - Raps / rybs  016-0, Stendyssegård\n",
      "3  2021  RA - Raps / rybs  016-0, Stendyssegård\n",
      "4  2022        HV - Hvede  037-0, Bispegård øst\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTION ---\n",
    "geojson, df_tasks = parse_isobus_taskdata(data_folder)\n",
    "\n",
    "if df_tasks is not None:\n",
    "    # Save CSV\n",
    "    csv_path = os.path.join(out_folder, 'task_index.csv')\n",
    "    df_tasks.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Save GeoJSON\n",
    "    geo_path = os.path.join(out_folder, 'harvest_tasks.geojson')\n",
    "    with open(geo_path, 'w') as f:\n",
    "        json.dump(geojson, f)\n",
    "        \n",
    "    print(f\"Success. Index saved to {csv_path}\")\n",
    "    print(f\"Sample:\\n{df_tasks[['Year', 'Crop', 'FieldName']].head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb3fbe-f7d5-47db-b8db-bdfcba69f27e",
   "metadata": {},
   "source": [
    "## Binary file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f986c2-6f20-466e-8636-2e5a37c24554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER: PACKET CLASSIFIER ---\n",
    "def classify_packet(gap_size):\n",
    "    \"\"\"\n",
    "    Determines the machine state based on the size of the proprietary header\n",
    "    preceding the data record.\n",
    "    \"\"\"\n",
    "    if gap_size == 25:\n",
    "        return \"Harvest (Mode A)\"\n",
    "    elif gap_size == 10:\n",
    "        return \"Transport (Mode B)\"\n",
    "    elif gap_size == 0:\n",
    "        return \"Contiguous\" # Rare, usually start of file\n",
    "    else:\n",
    "        return f\"Transition ({gap_size}b)\"\n",
    "\n",
    "# --- HELPER: DYNAMIC BOUNDING BOX ---\n",
    "def get_farm_bounding_box(taskdata_path):\n",
    "    print(f\"Reading Farm Geometry from: {taskdata_path}\")\n",
    "    try:\n",
    "        tree = ET.parse(taskdata_path)\n",
    "        root = tree.getroot()\n",
    "        lats, lons = [], []\n",
    "        for pnt in root.findall(\".//PFD//PNT\"):\n",
    "            try:\n",
    "                lats.append(float(pnt.attrib.get('C')))\n",
    "                lons.append(float(pnt.attrib.get('D')))\n",
    "            except: continue\n",
    "            \n",
    "        if not lats: return 54.5, 58.0, 8.0, 15.0\n",
    "        return min(lats)-GEO_BUFFER, max(lats)+GEO_BUFFER, min(lons)-GEO_BUFFER, max(lons)+GEO_BUFFER\n",
    "    except: return 54.5, 58.0, 8.0, 15.0\n",
    "\n",
    "# --- HELPER: METADATA & DEFS ---\n",
    "def load_metadata(taskdata_path):\n",
    "    try:\n",
    "        tree = ET.parse(taskdata_path)\n",
    "        root = tree.getroot()\n",
    "        pdt_map = {pdt.attrib.get('A'): pdt.attrib.get('B') for pdt in root.findall(\".//PDT\")}\n",
    "        meta_map = {}\n",
    "        for tsk in root.findall(\".//TSK\"):\n",
    "            tlg = tsk.find(\".//TLG\")\n",
    "            pan = tsk.find(\".//PAN\")\n",
    "            tim = tsk.find(\".//TIM\")\n",
    "            if tlg is not None:\n",
    "                log_id = tlg.attrib.get('A')\n",
    "                crop = pdt_map.get(pan.attrib.get('A'), 'Unknown') if pan is not None else 'Unknown'\n",
    "                start = tim.attrib.get('A') if tim is not None else None\n",
    "                meta_map[log_id] = {'Crop': crop, 'Start_Time': start}\n",
    "                meta_map[log_id + '.bin'] = {'Crop': crop, 'Start_Time': start}\n",
    "        return meta_map\n",
    "    except: return {}\n",
    "\n",
    "def get_dlv_defs(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        return [elem.attrib.get('A') for elem in tree.findall(\".//DLV\")]\n",
    "    except: return []\n",
    "\n",
    "# --- CORE: ENRICHED FORENSIC SCANNER ---\n",
    "def forensic_scan(bin_path, definitions, metadata, bounds):\n",
    "    min_lat, max_lat, min_lon, max_lon = bounds\n",
    "    filename = os.path.basename(bin_path)\n",
    "    meta = metadata.get(filename, {'Crop': 'Unknown', 'Start_Time': None})\n",
    "    \n",
    "    with open(bin_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    total_len = len(content)\n",
    "    cursor = 0\n",
    "    valid_rows = []\n",
    "    last_valid_end = 0\n",
    "\n",
    "    ddi_lookup = {'0054': 'Yield_Mass', '0095': 'Yield_Vol', '018D': 'Speed', '0063': 'Moisture'}\n",
    "\n",
    "    while cursor < total_len - 50:\n",
    "        match_found = False\n",
    "        bytes_consumed = 0\n",
    "        row_data = None\n",
    "\n",
    "        # Try Type 1 (32-bit)\n",
    "        try:\n",
    "            time_ms, lat_raw, lon_raw = struct.unpack('<Lii', content[cursor:cursor+12])\n",
    "            lat = lat_raw * 1e-7\n",
    "            lon = lon_raw * 1e-7\n",
    "            \n",
    "            if (min_lat < lat < max_lat) and (min_lon < lon < max_lon):\n",
    "                count = len(definitions)\n",
    "                payload_len = count * 4\n",
    "                if cursor + 12 + payload_len <= total_len:\n",
    "                    values = struct.unpack(f\"{count}i\", content[cursor+12 : cursor+12+payload_len])\n",
    "                    \n",
    "                    row_data = {'Time_ms': time_ms, 'Latitude': lat, 'Longitude': lon}\n",
    "                    for i, val in enumerate(values):\n",
    "                        ddi = definitions[i]\n",
    "                        row_data[ddi_lookup.get(ddi, f'DDI_{ddi}')] = val\n",
    "                    \n",
    "                    bytes_consumed = 12 + payload_len\n",
    "                    match_found = True\n",
    "        except: pass\n",
    "\n",
    "        if match_found:\n",
    "            # --- NEW: PACKET CLASSIFICATION ---\n",
    "            gap_size = cursor - last_valid_end\n",
    "            \n",
    "            # Enrich row with Forensic Info\n",
    "            row_data['File'] = filename\n",
    "            row_data['Crop'] = meta['Crop']\n",
    "            row_data['Start_Time_Str'] = meta['Start_Time']\n",
    "            row_data['Gap_Bytes'] = gap_size\n",
    "            row_data['Packet_Type'] = classify_packet(gap_size)\n",
    "\n",
    "            valid_rows.append(row_data)\n",
    "\n",
    "            cursor += bytes_consumed\n",
    "            last_valid_end = cursor\n",
    "        else:\n",
    "            cursor += 1\n",
    "\n",
    "    return pd.DataFrame(valid_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7f840d-1e69-4dd2-874d-9da0c44c24e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Farm Geometry from: ./data/taskdata_3\\TASKDATA.XML\n",
      "Starting Enriched Forensic Scan...\n",
      "Processing Metrics...\n",
      "\n",
      "SUCCESS.\n",
      "Dataset with Packet Types saved to: ./data/taskdata_3_out\\UNIVERSAL_ENRICHED_DATASET.csv\n",
      "Check the 'Packet_Type' column to see 'Harvest (Mode A)' vs 'Transport (Mode B)'\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTION ---\n",
    "taskdata_xml = os.path.join(data_folder, 'TASKDATA.XML')\n",
    "farm_bounds = get_farm_bounding_box(taskdata_xml)\n",
    "metadata_map = load_metadata(taskdata_xml)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "print(\"Starting Enriched Forensic Scan...\")\n",
    "\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.upper().startswith(\"TLG\") and filename.upper().endswith(\".BIN\"):\n",
    "        xml_full = os.path.join(data_folder, filename.rsplit('.', 1)[0] + \".xml\")\n",
    "        bin_full = os.path.join(data_folder, filename)\n",
    "        \n",
    "        if os.path.exists(xml_full):\n",
    "            \n",
    "            defs = get_dlv_defs(xml_full)\n",
    "            \n",
    "            df = forensic_scan(bin_full, defs, metadata_map, farm_bounds)\n",
    " \n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "\n",
    "# --- POST-PROCESSING ---\n",
    "if all_data:\n",
    "    print(\"Processing Metrics...\")\n",
    "    master = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Standard Post-Processing (Time, Speed, Yield, Density)\n",
    "    master['Task_Start'] = pd.to_datetime(master['Start_Time_Str'])\n",
    "    master = master.sort_values(['File', 'Time_ms'])\n",
    "    master['Time_Fix'] = master['Time_ms'] + master.groupby(['File', 'Time_ms']).cumcount() * 10\n",
    "    master['Datetime'] = master['Task_Start'] + pd.to_timedelta(master['Time_Fix'], unit='ms')\n",
    "\n",
    "    master['Raw_Mass'] = pd.to_numeric(master['Yield_Mass'], errors='coerce')\n",
    "    master['Raw_Vol']  = pd.to_numeric(master['Yield_Vol'], errors='coerce')\n",
    "    master['Status_Code'] = master['Raw_Mass'].fillna(0).astype(int) & 0xFF\n",
    "    master['Density_kg_L'] = np.where(master['Raw_Vol'] > 1000, (master['Raw_Mass'] / master['Raw_Vol']) * 10.0, 0)\n",
    "\n",
    "    # GPS Speed\n",
    "    lat_rad = np.radians(master['Latitude'])\n",
    "    dlat = master.groupby('File')['Latitude'].diff() * 111132\n",
    "    dlon = master.groupby('File')['Longitude'].diff() * 111132 * np.cos(lat_rad)\n",
    "    dist = np.sqrt(dlat**2 + dlon**2)\n",
    "    dt = master.groupby('File')['Time_Fix'].diff() / 1000.0\n",
    "    master['GPS_Speed'] = dist / dt\n",
    "    master['GPS_Speed'] = master['GPS_Speed'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    master['GPS_Speed'] = master['GPS_Speed'].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "    # Yield\n",
    "    HEADER_WIDTH = 9.0\n",
    "    master['Mass_kg_s'] = master['Raw_Mass'] / 1_000_000\n",
    "    master['Yield_T_Ha'] = (master['Mass_kg_s']) / (master['GPS_Speed'] * HEADER_WIDTH)\n",
    "    master['Yield_T_Ha'] = master['Yield_T_Ha'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # --- SAVING ---\n",
    "    out_csv = os.path.join(out_folder, 'UNIVERSAL_ENRICHED_DATASET.csv')\n",
    "    \n",
    "    # Include the new forensic columns\n",
    "    cols = ['Datetime', 'Latitude', 'Longitude', 'File', 'Crop', \n",
    "            'Packet_Type', 'Gap_Bytes',   # <-- NEW COLUMNS\n",
    "            'Yield_T_Ha', 'Density_kg_L', 'GPS_Speed', 'Status_Code', \n",
    "            'Raw_Mass', 'Raw_Vol']\n",
    "            \n",
    "    master[cols].to_csv(out_csv, index=False)\n",
    "    \n",
    "    print(f\"\\nSUCCESS.\")\n",
    "    print(f\"Dataset with Packet Types saved to: {out_csv}\")\n",
    "    print(\"Check the 'Packet_Type' column to see 'Harvest (Mode A)' vs 'Transport (Mode B)'\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3050d7-bb91-468f-b9f5-0c22b817878e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
